{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM,Dropout,GlobalAveragePooling2D,GlobalAveragePooling3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WIN_20180926_16_54_08_Pro_Right_Swipe_new;Right_Swipe_new;1\\n'\n",
      " 'WIN_20180925_18_02_58_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n'\n",
      " 'WIN_20180925_17_33_08_Pro_Left_Swipe_new;Left_Swipe_new;0\\n'\n",
      " 'WIN_20180925_17_51_17_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'\n",
      " 'WIN_20180926_17_17_35_Pro_Left_Swipe_new;Left_Swipe_new;0\\n'\n",
      " 'WIN_20180926_17_30_47_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'\n",
      " 'WIN_20180926_17_29_23_Pro_Stop_new;Stop_new;2\\n'\n",
      " 'WIN_20180926_17_12_26_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'\n",
      " 'WIN_20180926_17_29_01_Pro_Stop_new;Stop_new;2\\n'\n",
      " 'WIN_20180907_16_32_11_Pro_Stop Gesture_new;Stop Gesture_new;2\\n']\n"
     ]
    }
   ],
   "source": [
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
    "batch_size = 30\n",
    "image_len=100;\n",
    "image_wid=100;\n",
    "print(train_doc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [*range(0,30,2)]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size;\n",
    "#         print(\"Total batches:\"+str(num_batches))\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "#             print(\"In batch\"+str(batch))\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),image_len,image_wid,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "#                 print(\"index accessed:\"+str(folder + (batch*batch_size)))\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])# read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
    "                    image = imresize(image,(image_len,image_wid))\n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 \n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if len(t)%batch_size==0: continue\n",
    "#         print(\"in part-2\")\n",
    "        batch_size2 = len(t) - batch_size*num_batches\n",
    "#         print(\"batchSize2: \"+str(batch_size2))\n",
    "        batch_data = np.zeros((batch_size,len(img_idx),image_len,image_wid,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "        batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "        for folder in range(batch_size2): # iterate over the batch_size\n",
    "#             print(\"index accessed:\"+str(folder + (num_batches*batch_size)))\n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "            for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
    "                image = imresize(image,(image_len,image_wid))\n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                batch_data[folder,idx,:,:,0] = image[:,:,0]/255\n",
    "                batch_data[folder,idx,:,:,1] = image[:,:,1]/255\n",
    "                batch_data[folder,idx,:,:,2] = image[:,:,2]/255\n",
    "                    \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Iteration-1---------------\n",
      "Source path =  ./Project_data/val ; batch size = 30\n",
      "-------------Iteration-2---------------\n",
      "-------------Iteration-3---------------\n",
      "-------------Iteration-4---------------\n",
      "-------------Iteration-5---------------\n",
      "-------------Iteration-6---------------\n",
      "-------------Iteration-7---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[[0.37647059, 0.14509804, 0.08235294],\n",
       "           [0.38823529, 0.15686275, 0.08627451],\n",
       "           [0.40392157, 0.16078431, 0.08235294],\n",
       "           ...,\n",
       "           [0.43921569, 0.28235294, 0.19607843],\n",
       "           [0.43137255, 0.2745098 , 0.18823529],\n",
       "           [0.43137255, 0.2745098 , 0.19215686]],\n",
       " \n",
       "          [[0.37647059, 0.14509804, 0.09411765],\n",
       "           [0.38431373, 0.15686275, 0.09803922],\n",
       "           [0.40784314, 0.16470588, 0.09411765],\n",
       "           ...,\n",
       "           [0.43529412, 0.28627451, 0.19215686],\n",
       "           [0.43529412, 0.28627451, 0.19215686],\n",
       "           [0.43529412, 0.28627451, 0.18823529]],\n",
       " \n",
       "          [[0.34117647, 0.1254902 , 0.09019608],\n",
       "           [0.34901961, 0.1372549 , 0.09019608],\n",
       "           [0.38039216, 0.15294118, 0.09019608],\n",
       "           ...,\n",
       "           [0.42352941, 0.28235294, 0.17647059],\n",
       "           [0.42745098, 0.28235294, 0.17647059],\n",
       "           [0.42745098, 0.28235294, 0.16862745]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.24313725, 0.15686275, 0.10980392],\n",
       "           [0.23921569, 0.15294118, 0.10980392],\n",
       "           [0.23529412, 0.14901961, 0.10980392],\n",
       "           ...,\n",
       "           [0.41176471, 0.12941176, 0.09411765],\n",
       "           [0.43529412, 0.12156863, 0.08235294],\n",
       "           [0.43529412, 0.1254902 , 0.07058824]],\n",
       " \n",
       "          [[0.18431373, 0.1372549 , 0.10980392],\n",
       "           [0.18823529, 0.1372549 , 0.11372549],\n",
       "           [0.18823529, 0.1372549 , 0.11764706],\n",
       "           ...,\n",
       "           [0.40784314, 0.12941176, 0.08627451],\n",
       "           [0.42745098, 0.12156863, 0.08235294],\n",
       "           [0.42745098, 0.1254902 , 0.07058824]],\n",
       " \n",
       "          [[0.15686275, 0.14117647, 0.1254902 ],\n",
       "           [0.16470588, 0.14117647, 0.12156863],\n",
       "           [0.16078431, 0.13333333, 0.1254902 ],\n",
       "           ...,\n",
       "           [0.40392157, 0.13333333, 0.08627451],\n",
       "           [0.42352941, 0.1254902 , 0.08235294],\n",
       "           [0.41960784, 0.13333333, 0.07058824]]],\n",
       " \n",
       " \n",
       "         [[[0.3372549 , 0.1372549 , 0.07058824],\n",
       "           [0.37254902, 0.14901961, 0.08235294],\n",
       "           [0.41176471, 0.16078431, 0.07843137],\n",
       "           ...,\n",
       "           [0.41568627, 0.27843137, 0.17254902],\n",
       "           [0.41176471, 0.27843137, 0.16862745],\n",
       "           [0.40392157, 0.27058824, 0.16078431]],\n",
       " \n",
       "          [[0.3254902 , 0.13333333, 0.0745098 ],\n",
       "           [0.35294118, 0.14509804, 0.08235294],\n",
       "           [0.39607843, 0.15294118, 0.08235294],\n",
       "           ...,\n",
       "           [0.41568627, 0.2745098 , 0.17254902],\n",
       "           [0.41568627, 0.27843137, 0.16862745],\n",
       "           [0.41568627, 0.28235294, 0.16862745]],\n",
       " \n",
       "          [[0.29411765, 0.12156863, 0.07843137],\n",
       "           [0.31764706, 0.1254902 , 0.0745098 ],\n",
       "           [0.36078431, 0.1372549 , 0.08235294],\n",
       "           ...,\n",
       "           [0.40784314, 0.27058824, 0.16470588],\n",
       "           [0.4       , 0.26666667, 0.15294118],\n",
       "           [0.39607843, 0.27058824, 0.14901961]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.21960784, 0.15294118, 0.10588235],\n",
       "           [0.21960784, 0.14901961, 0.10196078],\n",
       "           [0.21960784, 0.14901961, 0.10196078],\n",
       "           ...,\n",
       "           [0.43529412, 0.13333333, 0.09411765],\n",
       "           [0.41176471, 0.11764706, 0.07058824],\n",
       "           [0.40784314, 0.12941176, 0.07058824]],\n",
       " \n",
       "          [[0.17647059, 0.13333333, 0.10588235],\n",
       "           [0.18039216, 0.13333333, 0.10980392],\n",
       "           [0.18039216, 0.13333333, 0.10980392],\n",
       "           ...,\n",
       "           [0.41960784, 0.1254902 , 0.08235294],\n",
       "           [0.39215686, 0.10980392, 0.05882353],\n",
       "           [0.38823529, 0.12156863, 0.05490196]],\n",
       " \n",
       "          [[0.15686275, 0.1254902 , 0.12156863],\n",
       "           [0.15686275, 0.12156863, 0.12156863],\n",
       "           [0.15686275, 0.1254902 , 0.12156863],\n",
       "           ...,\n",
       "           [0.40784314, 0.11764706, 0.07058824],\n",
       "           [0.38039216, 0.10980392, 0.05490196],\n",
       "           [0.38431373, 0.12941176, 0.05882353]]],\n",
       " \n",
       " \n",
       "         [[[0.37254902, 0.15294118, 0.09019608],\n",
       "           [0.38823529, 0.15686275, 0.08235294],\n",
       "           [0.41176471, 0.16078431, 0.08235294],\n",
       "           ...,\n",
       "           [0.43137255, 0.27843137, 0.19215686],\n",
       "           [0.41960784, 0.27058824, 0.18431373],\n",
       "           [0.41568627, 0.25882353, 0.17647059]],\n",
       " \n",
       "          [[0.35294118, 0.1372549 , 0.07843137],\n",
       "           [0.37647059, 0.14509804, 0.08627451],\n",
       "           [0.40784314, 0.15686275, 0.09019608],\n",
       "           ...,\n",
       "           [0.42352941, 0.27843137, 0.17254902],\n",
       "           [0.41960784, 0.27058824, 0.16862745],\n",
       "           [0.42745098, 0.27058824, 0.17254902]],\n",
       " \n",
       "          [[0.31372549, 0.11372549, 0.05882353],\n",
       "           [0.3372549 , 0.11764706, 0.0627451 ],\n",
       "           [0.37647059, 0.1372549 , 0.0745098 ],\n",
       "           ...,\n",
       "           [0.41568627, 0.2745098 , 0.15294118],\n",
       "           [0.41176471, 0.27058824, 0.14901961],\n",
       "           [0.41960784, 0.26666667, 0.15686275]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.23137255, 0.14509804, 0.09411765],\n",
       "           [0.23137255, 0.14509804, 0.10196078],\n",
       "           [0.23137255, 0.14117647, 0.10196078],\n",
       "           ...,\n",
       "           [0.39607843, 0.10980392, 0.0627451 ],\n",
       "           [0.40784314, 0.10980392, 0.07058824],\n",
       "           [0.41960784, 0.10588235, 0.0745098 ]],\n",
       " \n",
       "          [[0.18823529, 0.13333333, 0.10980392],\n",
       "           [0.18823529, 0.13333333, 0.11372549],\n",
       "           [0.18823529, 0.13333333, 0.11372549],\n",
       "           ...,\n",
       "           [0.38823529, 0.10980392, 0.05490196],\n",
       "           [0.40784314, 0.11372549, 0.0627451 ],\n",
       "           [0.41960784, 0.11372549, 0.06666667]],\n",
       " \n",
       "          [[0.15686275, 0.1254902 , 0.1254902 ],\n",
       "           [0.15686275, 0.12941176, 0.1254902 ],\n",
       "           [0.15294118, 0.1254902 , 0.12156863],\n",
       "           ...,\n",
       "           [0.38431373, 0.12156863, 0.04705882],\n",
       "           [0.4       , 0.12156863, 0.05490196],\n",
       "           [0.40784314, 0.11372549, 0.05490196]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.33333333, 0.12941176, 0.07843137],\n",
       "           [0.36470588, 0.13333333, 0.0745098 ],\n",
       "           [0.41176471, 0.14901961, 0.07843137],\n",
       "           ...,\n",
       "           [0.40784314, 0.2627451 , 0.16470588],\n",
       "           [0.4       , 0.2627451 , 0.16862745],\n",
       "           [0.39215686, 0.25490196, 0.16470588]],\n",
       " \n",
       "          [[0.31764706, 0.12941176, 0.08235294],\n",
       "           [0.34901961, 0.13333333, 0.07843137],\n",
       "           [0.39607843, 0.14901961, 0.08235294],\n",
       "           ...,\n",
       "           [0.40392157, 0.2627451 , 0.16470588],\n",
       "           [0.40392157, 0.2627451 , 0.16470588],\n",
       "           [0.40784314, 0.26666667, 0.17254902]],\n",
       " \n",
       "          [[0.27843137, 0.11372549, 0.06666667],\n",
       "           [0.29803922, 0.11764706, 0.06666667],\n",
       "           [0.35294118, 0.12941176, 0.07058824],\n",
       "           ...,\n",
       "           [0.39607843, 0.25882353, 0.15686275],\n",
       "           [0.39215686, 0.25098039, 0.15294118],\n",
       "           [0.39607843, 0.25098039, 0.15294118]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.21176471, 0.14509804, 0.09019608],\n",
       "           [0.20784314, 0.14117647, 0.09019608],\n",
       "           [0.20784314, 0.1372549 , 0.08627451],\n",
       "           ...,\n",
       "           [0.34901961, 0.10588235, 0.06666667],\n",
       "           [0.37254902, 0.11764706, 0.0627451 ],\n",
       "           [0.39215686, 0.10980392, 0.04313725]],\n",
       " \n",
       "          [[0.16862745, 0.12941176, 0.10196078],\n",
       "           [0.16862745, 0.1254902 , 0.09803922],\n",
       "           [0.16470588, 0.12156863, 0.09411765],\n",
       "           ...,\n",
       "           [0.33333333, 0.10588235, 0.0627451 ],\n",
       "           [0.36470588, 0.12156863, 0.05882353],\n",
       "           [0.38039216, 0.11372549, 0.04705882]],\n",
       " \n",
       "          [[0.14901961, 0.12156863, 0.11764706],\n",
       "           [0.14509804, 0.11764706, 0.11372549],\n",
       "           [0.14117647, 0.11372549, 0.10980392],\n",
       "           ...,\n",
       "           [0.32156863, 0.11372549, 0.05098039],\n",
       "           [0.35294118, 0.1254902 , 0.05098039],\n",
       "           [0.36862745, 0.11764706, 0.04313725]]],\n",
       " \n",
       " \n",
       "         [[[0.36862745, 0.14509804, 0.06666667],\n",
       "           [0.39215686, 0.14901961, 0.07843137],\n",
       "           [0.41568627, 0.16470588, 0.08235294],\n",
       "           ...,\n",
       "           [0.40784314, 0.2745098 , 0.14901961],\n",
       "           [0.39607843, 0.26666667, 0.14117647],\n",
       "           [0.39607843, 0.2627451 , 0.14117647]],\n",
       " \n",
       "          [[0.34117647, 0.12941176, 0.05882353],\n",
       "           [0.36078431, 0.1372549 , 0.07058824],\n",
       "           [0.39607843, 0.14901961, 0.07843137],\n",
       "           ...,\n",
       "           [0.40784314, 0.27058824, 0.15294118],\n",
       "           [0.4       , 0.26666667, 0.15294118],\n",
       "           [0.40784314, 0.27058824, 0.15686275]],\n",
       " \n",
       "          [[0.29411765, 0.11764706, 0.05098039],\n",
       "           [0.31764706, 0.12156863, 0.05882353],\n",
       "           [0.36078431, 0.12941176, 0.06666667],\n",
       "           ...,\n",
       "           [0.4       , 0.27058824, 0.15294118],\n",
       "           [0.39607843, 0.26666667, 0.15686275],\n",
       "           [0.4       , 0.26666667, 0.15686275]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.22745098, 0.14509804, 0.09803922],\n",
       "           [0.22745098, 0.14509804, 0.09411765],\n",
       "           [0.22352941, 0.14509804, 0.09411765],\n",
       "           ...,\n",
       "           [0.4       , 0.12941176, 0.08627451],\n",
       "           [0.40784314, 0.13333333, 0.08627451],\n",
       "           [0.38039216, 0.13333333, 0.0745098 ]],\n",
       " \n",
       "          [[0.17254902, 0.13333333, 0.09411765],\n",
       "           [0.17254902, 0.1372549 , 0.09411765],\n",
       "           [0.17254902, 0.13333333, 0.09803922],\n",
       "           ...,\n",
       "           [0.40784314, 0.11764706, 0.07843137],\n",
       "           [0.4       , 0.11764706, 0.0745098 ],\n",
       "           [0.36470588, 0.11764706, 0.05882353]],\n",
       " \n",
       "          [[0.13333333, 0.1254902 , 0.09019608],\n",
       "           [0.1372549 , 0.12941176, 0.09411765],\n",
       "           [0.1372549 , 0.1254902 , 0.09803922],\n",
       "           ...,\n",
       "           [0.41568627, 0.10196078, 0.06666667],\n",
       "           [0.4       , 0.10196078, 0.06666667],\n",
       "           [0.36078431, 0.10980392, 0.05098039]]],\n",
       " \n",
       " \n",
       "         [[[0.36470588, 0.14509804, 0.0627451 ],\n",
       "           [0.39215686, 0.15294118, 0.0745098 ],\n",
       "           [0.41568627, 0.16470588, 0.0745098 ],\n",
       "           ...,\n",
       "           [0.40784314, 0.27058824, 0.15294118],\n",
       "           [0.4       , 0.26666667, 0.14509804],\n",
       "           [0.39607843, 0.25882353, 0.14509804]],\n",
       " \n",
       "          [[0.3372549 , 0.12941176, 0.0627451 ],\n",
       "           [0.35686275, 0.13333333, 0.0627451 ],\n",
       "           [0.4       , 0.14901961, 0.07843137],\n",
       "           ...,\n",
       "           [0.4       , 0.26666667, 0.14901961],\n",
       "           [0.39607843, 0.25882353, 0.14901961],\n",
       "           [0.39607843, 0.2627451 , 0.14901961]],\n",
       " \n",
       "          [[0.28627451, 0.10980392, 0.05098039],\n",
       "           [0.31372549, 0.11764706, 0.05882353],\n",
       "           [0.36078431, 0.12941176, 0.06666667],\n",
       "           ...,\n",
       "           [0.39607843, 0.2627451 , 0.14901961],\n",
       "           [0.39607843, 0.2627451 , 0.15686275],\n",
       "           [0.39215686, 0.26666667, 0.15686275]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.22352941, 0.14509804, 0.09411765],\n",
       "           [0.22352941, 0.14509804, 0.09019608],\n",
       "           [0.21568627, 0.14509804, 0.09019608],\n",
       "           ...,\n",
       "           [0.41176471, 0.11764706, 0.07843137],\n",
       "           [0.42352941, 0.1254902 , 0.09019608],\n",
       "           [0.39215686, 0.1254902 , 0.0745098 ]],\n",
       " \n",
       "          [[0.16862745, 0.12941176, 0.09411765],\n",
       "           [0.16862745, 0.13333333, 0.09019608],\n",
       "           [0.16862745, 0.13333333, 0.09411765],\n",
       "           ...,\n",
       "           [0.41960784, 0.10588235, 0.06666667],\n",
       "           [0.41176471, 0.10980392, 0.07058824],\n",
       "           [0.38039216, 0.10980392, 0.0627451 ]],\n",
       " \n",
       "          [[0.1372549 , 0.12156863, 0.09803922],\n",
       "           [0.13333333, 0.1254902 , 0.09019608],\n",
       "           [0.13333333, 0.12156863, 0.09411765],\n",
       "           ...,\n",
       "           [0.42352941, 0.09411765, 0.05490196],\n",
       "           [0.41176471, 0.09411765, 0.05882353],\n",
       "           [0.37647059, 0.10196078, 0.05882353]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[0.43137255, 0.30980392, 0.27058824],\n",
       "           [0.4627451 , 0.34901961, 0.30196078],\n",
       "           [0.50980392, 0.39607843, 0.3372549 ],\n",
       "           ...,\n",
       "           [0.98823529, 0.94509804, 0.96078431],\n",
       "           [0.99607843, 0.96862745, 0.98039216],\n",
       "           [1.        , 0.98823529, 0.99215686]],\n",
       " \n",
       "          [[0.41568627, 0.30196078, 0.25490196],\n",
       "           [0.43529412, 0.32156863, 0.26666667],\n",
       "           [0.4627451 , 0.34901961, 0.29411765],\n",
       "           ...,\n",
       "           [0.91764706, 0.85490196, 0.85882353],\n",
       "           [0.94901961, 0.89803922, 0.89803922],\n",
       "           [0.96862745, 0.92941176, 0.92941176]],\n",
       " \n",
       "          [[0.39215686, 0.28627451, 0.23137255],\n",
       "           [0.39607843, 0.29019608, 0.23137255],\n",
       "           [0.4       , 0.29019608, 0.23137255],\n",
       "           ...,\n",
       "           [0.86666667, 0.78039216, 0.76470588],\n",
       "           [0.88235294, 0.80784314, 0.78431373],\n",
       "           [0.89803922, 0.82745098, 0.80784314]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.43529412, 0.2745098 , 0.16078431],\n",
       "           [0.42745098, 0.2745098 , 0.15686275],\n",
       "           [0.42745098, 0.2745098 , 0.15686275],\n",
       "           ...,\n",
       "           [0.66666667, 0.5254902 , 0.41568627],\n",
       "           [0.4       , 0.27058824, 0.16862745],\n",
       "           [0.30588235, 0.18431373, 0.09019608]],\n",
       " \n",
       "          [[0.42745098, 0.26666667, 0.15686275],\n",
       "           [0.42352941, 0.27058824, 0.15686275],\n",
       "           [0.42352941, 0.2745098 , 0.15686275],\n",
       "           ...,\n",
       "           [0.70980392, 0.58039216, 0.47058824],\n",
       "           [0.37254902, 0.25490196, 0.15294118],\n",
       "           [0.21568627, 0.10980392, 0.01960784]],\n",
       " \n",
       "          [[0.44705882, 0.28235294, 0.17647059],\n",
       "           [0.43921569, 0.28235294, 0.17254902],\n",
       "           [0.43921569, 0.28627451, 0.17254902],\n",
       "           ...,\n",
       "           [0.62745098, 0.51372549, 0.4       ],\n",
       "           [0.33333333, 0.22745098, 0.12156863],\n",
       "           [0.17647059, 0.09019608, 0.00392157]]],\n",
       " \n",
       " \n",
       "         [[[0.43137255, 0.31372549, 0.2745098 ],\n",
       "           [0.4745098 , 0.35686275, 0.30980392],\n",
       "           [0.51764706, 0.4       , 0.34509804],\n",
       "           ...,\n",
       "           [0.98431373, 0.95294118, 0.96470588],\n",
       "           [0.99215686, 0.97254902, 0.98039216],\n",
       "           [0.99607843, 0.98823529, 0.99607843]],\n",
       " \n",
       "          [[0.41568627, 0.29803922, 0.25490196],\n",
       "           [0.43921569, 0.3254902 , 0.27058824],\n",
       "           [0.4745098 , 0.36078431, 0.30588235],\n",
       "           ...,\n",
       "           [0.90980392, 0.85490196, 0.85882353],\n",
       "           [0.9372549 , 0.90196078, 0.89411765],\n",
       "           [0.96862745, 0.9372549 , 0.9372549 ]],\n",
       " \n",
       "          [[0.4       , 0.28235294, 0.22745098],\n",
       "           [0.39607843, 0.29019608, 0.22745098],\n",
       "           [0.40392157, 0.29019608, 0.23529412],\n",
       "           ...,\n",
       "           [0.85490196, 0.78039216, 0.76470588],\n",
       "           [0.87058824, 0.80392157, 0.78039216],\n",
       "           [0.89803922, 0.82745098, 0.81176471]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.44313725, 0.28235294, 0.16862745],\n",
       "           [0.43529412, 0.28235294, 0.17254902],\n",
       "           [0.43137255, 0.2745098 , 0.16862745],\n",
       "           ...,\n",
       "           [0.65882353, 0.5254902 , 0.4       ],\n",
       "           [0.38823529, 0.25882353, 0.14901961],\n",
       "           [0.29019608, 0.17254902, 0.07843137]],\n",
       " \n",
       "          [[0.43921569, 0.27843137, 0.16862745],\n",
       "           [0.43529412, 0.27843137, 0.16470588],\n",
       "           [0.43137255, 0.2745098 , 0.16470588],\n",
       "           ...,\n",
       "           [0.70588235, 0.58039216, 0.4627451 ],\n",
       "           [0.37647059, 0.25882353, 0.14901961],\n",
       "           [0.22352941, 0.11372549, 0.02352941]],\n",
       " \n",
       "          [[0.4627451 , 0.30196078, 0.19607843],\n",
       "           [0.45490196, 0.29411765, 0.18431373],\n",
       "           [0.45098039, 0.29411765, 0.18039216],\n",
       "           ...,\n",
       "           [0.63529412, 0.51764706, 0.4       ],\n",
       "           [0.34901961, 0.23921569, 0.12941176],\n",
       "           [0.20392157, 0.10588235, 0.01176471]]],\n",
       " \n",
       " \n",
       "         [[[0.43921569, 0.31764706, 0.27843137],\n",
       "           [0.48235294, 0.36470588, 0.31764706],\n",
       "           [0.51764706, 0.40392157, 0.34509804],\n",
       "           ...,\n",
       "           [0.99215686, 0.96078431, 0.97254902],\n",
       "           [0.99607843, 0.97647059, 0.98823529],\n",
       "           [0.99607843, 0.98823529, 0.99607843]],\n",
       " \n",
       "          [[0.42352941, 0.30588235, 0.2627451 ],\n",
       "           [0.44313725, 0.3254902 , 0.2745098 ],\n",
       "           [0.48235294, 0.36470588, 0.30980392],\n",
       "           ...,\n",
       "           [0.93333333, 0.87843137, 0.88235294],\n",
       "           [0.95686275, 0.91372549, 0.91372549],\n",
       "           [0.98039216, 0.94901961, 0.95294118]],\n",
       " \n",
       "          [[0.4       , 0.28627451, 0.23137255],\n",
       "           [0.40392157, 0.28627451, 0.22745098],\n",
       "           [0.40392157, 0.28627451, 0.23137255],\n",
       "           ...,\n",
       "           [0.86666667, 0.79215686, 0.78039216],\n",
       "           [0.88627451, 0.81960784, 0.80392157],\n",
       "           [0.91372549, 0.85098039, 0.83529412]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.43529412, 0.27058824, 0.16470588],\n",
       "           [0.43529412, 0.27058824, 0.16078431],\n",
       "           [0.42745098, 0.27058824, 0.16470588],\n",
       "           ...,\n",
       "           [0.6627451 , 0.53333333, 0.41176471],\n",
       "           [0.4       , 0.2745098 , 0.16862745],\n",
       "           [0.29803922, 0.18823529, 0.09411765]],\n",
       " \n",
       "          [[0.42745098, 0.26666667, 0.15294118],\n",
       "           [0.42745098, 0.26666667, 0.15686275],\n",
       "           [0.42352941, 0.27058824, 0.16470588],\n",
       "           ...,\n",
       "           [0.70588235, 0.58039216, 0.45882353],\n",
       "           [0.37254902, 0.25490196, 0.14509804],\n",
       "           [0.21960784, 0.11372549, 0.01960784]],\n",
       " \n",
       "          [[0.44705882, 0.28627451, 0.17254902],\n",
       "           [0.44705882, 0.28235294, 0.17647059],\n",
       "           [0.43921569, 0.28627451, 0.17254902],\n",
       "           ...,\n",
       "           [0.62352941, 0.51372549, 0.39215686],\n",
       "           [0.33333333, 0.22745098, 0.11372549],\n",
       "           [0.18431373, 0.09019608, 0.00392157]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.43921569, 0.3372549 , 0.26666667],\n",
       "           [0.47058824, 0.36470588, 0.30980392],\n",
       "           [0.50196078, 0.39607843, 0.34509804],\n",
       "           ...,\n",
       "           [0.98431373, 0.95686275, 0.96862745],\n",
       "           [0.98823529, 0.97254902, 0.98431373],\n",
       "           [0.99607843, 0.99607843, 0.99607843]],\n",
       " \n",
       "          [[0.40392157, 0.29803922, 0.22745098],\n",
       "           [0.42745098, 0.31764706, 0.25882353],\n",
       "           [0.45490196, 0.34901961, 0.29803922],\n",
       "           ...,\n",
       "           [0.90980392, 0.85882353, 0.85882353],\n",
       "           [0.94117647, 0.90588235, 0.89803922],\n",
       "           [0.96470588, 0.94117647, 0.92941176]],\n",
       " \n",
       "          [[0.4       , 0.28627451, 0.20784314],\n",
       "           [0.40392157, 0.29411765, 0.23137255],\n",
       "           [0.41176471, 0.30196078, 0.25490196],\n",
       "           ...,\n",
       "           [0.85490196, 0.77647059, 0.74901961],\n",
       "           [0.86666667, 0.8       , 0.76862745],\n",
       "           [0.89019608, 0.82352941, 0.79215686]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.44313725, 0.29411765, 0.17254902],\n",
       "           [0.43921569, 0.28627451, 0.16862745],\n",
       "           [0.43529412, 0.28235294, 0.16470588],\n",
       "           ...,\n",
       "           [0.6627451 , 0.5254902 , 0.40392157],\n",
       "           [0.38431373, 0.25882353, 0.14509804],\n",
       "           [0.26666667, 0.15686275, 0.0627451 ]],\n",
       " \n",
       "          [[0.44313725, 0.29411765, 0.17254902],\n",
       "           [0.44313725, 0.28627451, 0.17254902],\n",
       "           [0.44313725, 0.28627451, 0.17254902],\n",
       "           ...,\n",
       "           [0.70196078, 0.57254902, 0.45490196],\n",
       "           [0.36470588, 0.24705882, 0.14117647],\n",
       "           [0.20392157, 0.09803922, 0.01176471]],\n",
       " \n",
       "          [[0.4627451 , 0.30980392, 0.19215686],\n",
       "           [0.45882353, 0.30588235, 0.19607843],\n",
       "           [0.4627451 , 0.30980392, 0.19607843],\n",
       "           ...,\n",
       "           [0.61568627, 0.50588235, 0.39215686],\n",
       "           [0.33333333, 0.22745098, 0.11764706],\n",
       "           [0.19215686, 0.09803922, 0.00392157]]],\n",
       " \n",
       " \n",
       "         [[[0.43137255, 0.30980392, 0.27058824],\n",
       "           [0.4627451 , 0.34901961, 0.29803922],\n",
       "           [0.50980392, 0.39215686, 0.3372549 ],\n",
       "           ...,\n",
       "           [0.99215686, 0.95294118, 0.96862745],\n",
       "           [1.        , 0.98039216, 0.98823529],\n",
       "           [1.        , 0.99215686, 0.99607843]],\n",
       " \n",
       "          [[0.41568627, 0.30196078, 0.25490196],\n",
       "           [0.43529412, 0.32156863, 0.26666667],\n",
       "           [0.4627451 , 0.34901961, 0.29411765],\n",
       "           ...,\n",
       "           [0.9372549 , 0.87843137, 0.88627451],\n",
       "           [0.96470588, 0.9254902 , 0.9254902 ],\n",
       "           [0.98431373, 0.94901961, 0.94901961]],\n",
       " \n",
       "          [[0.39215686, 0.28627451, 0.22745098],\n",
       "           [0.39607843, 0.29019608, 0.22745098],\n",
       "           [0.4       , 0.29019608, 0.23137255],\n",
       "           ...,\n",
       "           [0.8745098 , 0.79607843, 0.78823529],\n",
       "           [0.89019608, 0.82745098, 0.81568627],\n",
       "           [0.91764706, 0.85098039, 0.83921569]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.42745098, 0.2745098 , 0.15686275],\n",
       "           [0.43137255, 0.2745098 , 0.16470588],\n",
       "           [0.42745098, 0.27058824, 0.15686275],\n",
       "           ...,\n",
       "           [0.65882353, 0.5254902 , 0.41176471],\n",
       "           [0.4       , 0.2745098 , 0.16862745],\n",
       "           [0.30196078, 0.18823529, 0.09411765]],\n",
       " \n",
       "          [[0.42352941, 0.26666667, 0.15686275],\n",
       "           [0.42745098, 0.26666667, 0.15686275],\n",
       "           [0.42745098, 0.27058824, 0.15686275],\n",
       "           ...,\n",
       "           [0.70980392, 0.58823529, 0.4745098 ],\n",
       "           [0.37254902, 0.25882353, 0.15294118],\n",
       "           [0.21960784, 0.11764706, 0.02352941]],\n",
       " \n",
       "          [[0.44705882, 0.28627451, 0.18039216],\n",
       "           [0.44313725, 0.28627451, 0.16862745],\n",
       "           [0.44313725, 0.28627451, 0.16862745],\n",
       "           ...,\n",
       "           [0.63529412, 0.52156863, 0.41176471],\n",
       "           [0.3254902 , 0.22745098, 0.12156863],\n",
       "           [0.18039216, 0.09019608, 0.00392157]]],\n",
       " \n",
       " \n",
       "         [[[0.43137255, 0.3254902 , 0.2627451 ],\n",
       "           [0.47058824, 0.36470588, 0.30196078],\n",
       "           [0.50980392, 0.40392157, 0.3372549 ],\n",
       "           ...,\n",
       "           [0.99607843, 0.95686275, 0.96078431],\n",
       "           [1.        , 0.96862745, 0.96470588],\n",
       "           [0.99215686, 0.97254902, 0.96862745]],\n",
       " \n",
       "          [[0.40392157, 0.29803922, 0.23529412],\n",
       "           [0.43137255, 0.3254902 , 0.2627451 ],\n",
       "           [0.48235294, 0.37254902, 0.30196078],\n",
       "           ...,\n",
       "           [0.95294118, 0.88627451, 0.88235294],\n",
       "           [0.97254902, 0.9254902 , 0.91372549],\n",
       "           [0.97647059, 0.94509804, 0.9372549 ]],\n",
       " \n",
       "          [[0.38039216, 0.2745098 , 0.2       ],\n",
       "           [0.40392157, 0.29411765, 0.21960784],\n",
       "           [0.41568627, 0.30196078, 0.22745098],\n",
       "           ...,\n",
       "           [0.8745098 , 0.78823529, 0.76470588],\n",
       "           [0.89803922, 0.81960784, 0.79215686],\n",
       "           [0.91372549, 0.83529412, 0.80784314]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.44705882, 0.29019608, 0.16470588],\n",
       "           [0.45098039, 0.28627451, 0.16470588],\n",
       "           [0.44313725, 0.27843137, 0.16862745],\n",
       "           ...,\n",
       "           [0.64705882, 0.5254902 , 0.38823529],\n",
       "           [0.36078431, 0.25098039, 0.12941176],\n",
       "           [0.25490196, 0.15294118, 0.07843137]],\n",
       " \n",
       "          [[0.44705882, 0.28627451, 0.16862745],\n",
       "           [0.45490196, 0.29019608, 0.16862745],\n",
       "           [0.45098039, 0.29019608, 0.17254902],\n",
       "           ...,\n",
       "           [0.70196078, 0.56862745, 0.44313725],\n",
       "           [0.35686275, 0.23137255, 0.12156863],\n",
       "           [0.19607843, 0.07843137, 0.01960784]],\n",
       " \n",
       "          [[0.45882353, 0.29803922, 0.18431373],\n",
       "           [0.4627451 , 0.29803922, 0.18039216],\n",
       "           [0.4627451 , 0.30196078, 0.18431373],\n",
       "           ...,\n",
       "           [0.65098039, 0.50588235, 0.38431373],\n",
       "           [0.36078431, 0.22352941, 0.11764706],\n",
       "           [0.2       , 0.08235294, 0.02352941]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[0.4745098 , 0.52941176, 0.54901961],\n",
       "           [0.48235294, 0.5372549 , 0.55686275],\n",
       "           [0.49019608, 0.54509804, 0.56470588],\n",
       "           ...,\n",
       "           [0.42352941, 0.42352941, 0.42352941],\n",
       "           [0.42352941, 0.42352941, 0.42352941],\n",
       "           [0.42352941, 0.42352941, 0.43137255]],\n",
       " \n",
       "          [[0.4745098 , 0.52941176, 0.55294118],\n",
       "           [0.47843137, 0.53333333, 0.55686275],\n",
       "           [0.49019608, 0.54509804, 0.56470588],\n",
       "           ...,\n",
       "           [0.44313725, 0.44313725, 0.44705882],\n",
       "           [0.44705882, 0.44705882, 0.45098039],\n",
       "           [0.44313725, 0.43529412, 0.45098039]],\n",
       " \n",
       "          [[0.48235294, 0.5372549 , 0.56470588],\n",
       "           [0.48627451, 0.54117647, 0.56078431],\n",
       "           [0.49019608, 0.54901961, 0.57254902],\n",
       "           ...,\n",
       "           [0.45882353, 0.45490196, 0.46666667],\n",
       "           [0.45882353, 0.45490196, 0.4627451 ],\n",
       "           [0.45882353, 0.44705882, 0.4627451 ]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.56470588, 0.50196078, 0.43137255],\n",
       "           [0.56862745, 0.50588235, 0.43529412],\n",
       "           [0.55294118, 0.48627451, 0.41568627],\n",
       "           ...,\n",
       "           [0.27058824, 0.2627451 , 0.26666667],\n",
       "           [0.27843137, 0.2627451 , 0.27058824],\n",
       "           [0.29411765, 0.2745098 , 0.29411765]],\n",
       " \n",
       "          [[0.6       , 0.52941176, 0.46666667],\n",
       "           [0.60392157, 0.5254902 , 0.46666667],\n",
       "           [0.58823529, 0.50980392, 0.44705882],\n",
       "           ...,\n",
       "           [0.29019608, 0.27058824, 0.28235294],\n",
       "           [0.29803922, 0.27058824, 0.28627451],\n",
       "           [0.30980392, 0.27843137, 0.30196078]],\n",
       " \n",
       "          [[0.59215686, 0.51372549, 0.4627451 ],\n",
       "           [0.59607843, 0.50980392, 0.45490196],\n",
       "           [0.57254902, 0.48627451, 0.43137255],\n",
       "           ...,\n",
       "           [0.29019608, 0.2627451 , 0.27843137],\n",
       "           [0.29019608, 0.25882353, 0.27058824],\n",
       "           [0.29019608, 0.2627451 , 0.27843137]]],\n",
       " \n",
       " \n",
       "         [[[0.52156863, 0.54509804, 0.57647059],\n",
       "           [0.52156863, 0.54509804, 0.58431373],\n",
       "           [0.53333333, 0.54901961, 0.59215686],\n",
       "           ...,\n",
       "           [0.45490196, 0.43921569, 0.44705882],\n",
       "           [0.45098039, 0.44313725, 0.43921569],\n",
       "           [0.44705882, 0.43921569, 0.43921569]],\n",
       " \n",
       "          [[0.52156863, 0.54117647, 0.57647059],\n",
       "           [0.52156863, 0.54509804, 0.58039216],\n",
       "           [0.52941176, 0.54509804, 0.58823529],\n",
       "           ...,\n",
       "           [0.46666667, 0.45098039, 0.4627451 ],\n",
       "           [0.4627451 , 0.45098039, 0.45882353],\n",
       "           [0.4627451 , 0.45098039, 0.45882353]],\n",
       " \n",
       "          [[0.52156863, 0.54117647, 0.58039216],\n",
       "           [0.51764706, 0.54509804, 0.58039216],\n",
       "           [0.5254902 , 0.54901961, 0.58823529],\n",
       "           ...,\n",
       "           [0.47843137, 0.4627451 , 0.47843137],\n",
       "           [0.4745098 , 0.45882353, 0.4745098 ],\n",
       "           [0.4745098 , 0.45882353, 0.4745098 ]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.59215686, 0.52941176, 0.45098039],\n",
       "           [0.58823529, 0.52941176, 0.45098039],\n",
       "           [0.56078431, 0.50588235, 0.41960784],\n",
       "           ...,\n",
       "           [0.27058824, 0.25098039, 0.27058824],\n",
       "           [0.28235294, 0.2627451 , 0.26666667],\n",
       "           [0.29019608, 0.28235294, 0.26666667]],\n",
       " \n",
       "          [[0.60392157, 0.53333333, 0.47058824],\n",
       "           [0.60392157, 0.53333333, 0.47058824],\n",
       "           [0.59215686, 0.52156863, 0.45098039],\n",
       "           ...,\n",
       "           [0.29019608, 0.27058824, 0.29019608],\n",
       "           [0.29411765, 0.2745098 , 0.2745098 ],\n",
       "           [0.29019608, 0.2745098 , 0.25882353]],\n",
       " \n",
       "          [[0.60392157, 0.5254902 , 0.48627451],\n",
       "           [0.61176471, 0.53333333, 0.48627451],\n",
       "           [0.59215686, 0.51764706, 0.45882353],\n",
       "           ...,\n",
       "           [0.28235294, 0.26666667, 0.2745098 ],\n",
       "           [0.28627451, 0.26666667, 0.25490196],\n",
       "           [0.27058824, 0.25490196, 0.23529412]]],\n",
       " \n",
       " \n",
       "         [[[0.4745098 , 0.52941176, 0.54901961],\n",
       "           [0.48235294, 0.5372549 , 0.55686275],\n",
       "           [0.49019608, 0.54509804, 0.56470588],\n",
       "           ...,\n",
       "           [0.41960784, 0.41960784, 0.42352941],\n",
       "           [0.41960784, 0.41960784, 0.42352941],\n",
       "           [0.42352941, 0.42352941, 0.43137255]],\n",
       " \n",
       "          [[0.4745098 , 0.52941176, 0.55294118],\n",
       "           [0.47843137, 0.53333333, 0.55686275],\n",
       "           [0.48627451, 0.54117647, 0.56470588],\n",
       "           ...,\n",
       "           [0.44313725, 0.43921569, 0.44705882],\n",
       "           [0.44313725, 0.44313725, 0.44705882],\n",
       "           [0.44313725, 0.43529412, 0.44705882]],\n",
       " \n",
       "          [[0.48235294, 0.5372549 , 0.56862745],\n",
       "           [0.48627451, 0.54117647, 0.56078431],\n",
       "           [0.49019608, 0.54509804, 0.57254902],\n",
       "           ...,\n",
       "           [0.45490196, 0.45490196, 0.4627451 ],\n",
       "           [0.45490196, 0.45098039, 0.45882353],\n",
       "           [0.45882353, 0.44705882, 0.45882353]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.57254902, 0.50588235, 0.42352941],\n",
       "           [0.56862745, 0.50588235, 0.43137255],\n",
       "           [0.54901961, 0.48627451, 0.40784314],\n",
       "           ...,\n",
       "           [0.27058824, 0.25882353, 0.26666667],\n",
       "           [0.2745098 , 0.2627451 , 0.26666667],\n",
       "           [0.29411765, 0.27843137, 0.28235294]],\n",
       " \n",
       "          [[0.60784314, 0.53333333, 0.45882353],\n",
       "           [0.60392157, 0.52941176, 0.4627451 ],\n",
       "           [0.58431373, 0.50980392, 0.43921569],\n",
       "           ...,\n",
       "           [0.29019608, 0.26666667, 0.28627451],\n",
       "           [0.29803922, 0.27058824, 0.28627451],\n",
       "           [0.30980392, 0.27843137, 0.29411765]],\n",
       " \n",
       "          [[0.59215686, 0.51764706, 0.45490196],\n",
       "           [0.59607843, 0.50980392, 0.45882353],\n",
       "           [0.57254902, 0.48627451, 0.43137255],\n",
       "           ...,\n",
       "           [0.29019608, 0.2627451 , 0.28627451],\n",
       "           [0.28627451, 0.25490196, 0.2745098 ],\n",
       "           [0.29411765, 0.2627451 , 0.28235294]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.47843137, 0.52941176, 0.55294118],\n",
       "           [0.48627451, 0.5372549 , 0.56078431],\n",
       "           [0.49411765, 0.54901961, 0.56862745],\n",
       "           ...,\n",
       "           [0.42745098, 0.42352941, 0.43137255],\n",
       "           [0.42745098, 0.42745098, 0.42745098],\n",
       "           [0.42745098, 0.42745098, 0.43529412]],\n",
       " \n",
       "          [[0.47843137, 0.52941176, 0.55686275],\n",
       "           [0.48235294, 0.5372549 , 0.56078431],\n",
       "           [0.49411765, 0.54901961, 0.56470588],\n",
       "           ...,\n",
       "           [0.44313725, 0.43921569, 0.45490196],\n",
       "           [0.44705882, 0.44313725, 0.45490196],\n",
       "           [0.44313725, 0.43921569, 0.44705882]],\n",
       " \n",
       "          [[0.48627451, 0.5372549 , 0.56862745],\n",
       "           [0.49019608, 0.54509804, 0.56470588],\n",
       "           [0.49411765, 0.54901961, 0.57254902],\n",
       "           ...,\n",
       "           [0.45882353, 0.45490196, 0.4745098 ],\n",
       "           [0.45882353, 0.45098039, 0.47058824],\n",
       "           [0.45882353, 0.44705882, 0.45882353]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.56862745, 0.50588235, 0.42352941],\n",
       "           [0.57254902, 0.51372549, 0.42745098],\n",
       "           [0.55294118, 0.49019608, 0.41176471],\n",
       "           ...,\n",
       "           [0.27843137, 0.2627451 , 0.2745098 ],\n",
       "           [0.27843137, 0.2627451 , 0.27843137],\n",
       "           [0.29411765, 0.27843137, 0.28627451]],\n",
       " \n",
       "          [[0.61176471, 0.52941176, 0.46666667],\n",
       "           [0.60392157, 0.52941176, 0.4627451 ],\n",
       "           [0.58823529, 0.50980392, 0.44705882],\n",
       "           ...,\n",
       "           [0.29411765, 0.27058824, 0.28235294],\n",
       "           [0.30196078, 0.2745098 , 0.28627451],\n",
       "           [0.30588235, 0.28627451, 0.29411765]],\n",
       " \n",
       "          [[0.6       , 0.51372549, 0.4627451 ],\n",
       "           [0.59215686, 0.51372549, 0.4627451 ],\n",
       "           [0.57254902, 0.49019608, 0.43529412],\n",
       "           ...,\n",
       "           [0.29411765, 0.27058824, 0.27843137],\n",
       "           [0.29019608, 0.2627451 , 0.27058824],\n",
       "           [0.29019608, 0.27058824, 0.27843137]]],\n",
       " \n",
       " \n",
       "         [[[0.52156863, 0.54509804, 0.58431373],\n",
       "           [0.5254902 , 0.54509804, 0.58823529],\n",
       "           [0.53333333, 0.54901961, 0.58823529],\n",
       "           ...,\n",
       "           [0.45098039, 0.43529412, 0.45490196],\n",
       "           [0.44313725, 0.43137255, 0.43921569],\n",
       "           [0.44313725, 0.43137255, 0.44705882]],\n",
       " \n",
       "          [[0.5254902 , 0.54509804, 0.58039216],\n",
       "           [0.5254902 , 0.54509804, 0.58431373],\n",
       "           [0.53333333, 0.54901961, 0.58823529],\n",
       "           ...,\n",
       "           [0.4745098 , 0.45882353, 0.47843137],\n",
       "           [0.47058824, 0.45882353, 0.47058824],\n",
       "           [0.47058824, 0.45882353, 0.4745098 ]],\n",
       " \n",
       "          [[0.52941176, 0.54117647, 0.57254902],\n",
       "           [0.5254902 , 0.54509804, 0.58431373],\n",
       "           [0.5372549 , 0.55294118, 0.59215686],\n",
       "           ...,\n",
       "           [0.4745098 , 0.4627451 , 0.47843137],\n",
       "           [0.4745098 , 0.4627451 , 0.4745098 ],\n",
       "           [0.47058824, 0.45882353, 0.47058824]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.61176471, 0.5372549 , 0.44705882],\n",
       "           [0.6       , 0.52156863, 0.43529412],\n",
       "           [0.57647059, 0.49803922, 0.41176471],\n",
       "           ...,\n",
       "           [0.29019608, 0.26666667, 0.29019608],\n",
       "           [0.30588235, 0.27058824, 0.29019608],\n",
       "           [0.3254902 , 0.28627451, 0.29803922]],\n",
       " \n",
       "          [[0.61568627, 0.55294118, 0.45490196],\n",
       "           [0.61568627, 0.54509804, 0.45882353],\n",
       "           [0.6       , 0.52941176, 0.44313725],\n",
       "           ...,\n",
       "           [0.31372549, 0.28235294, 0.30588235],\n",
       "           [0.31764706, 0.2745098 , 0.29411765],\n",
       "           [0.33333333, 0.29019608, 0.29803922]],\n",
       " \n",
       "          [[0.60392157, 0.55294118, 0.45098039],\n",
       "           [0.59607843, 0.53333333, 0.44705882],\n",
       "           [0.57647059, 0.52156863, 0.43529412],\n",
       "           ...,\n",
       "           [0.30196078, 0.25882353, 0.29019608],\n",
       "           [0.31372549, 0.25882353, 0.27843137],\n",
       "           [0.32156863, 0.2745098 , 0.28235294]]],\n",
       " \n",
       " \n",
       "         [[[0.54117647, 0.54117647, 0.59215686],\n",
       "           [0.5372549 , 0.54117647, 0.59215686],\n",
       "           [0.54117647, 0.54901961, 0.59607843],\n",
       "           ...,\n",
       "           [0.45882353, 0.44313725, 0.45098039],\n",
       "           [0.45098039, 0.44313725, 0.44705882],\n",
       "           [0.45490196, 0.44313725, 0.45490196]],\n",
       " \n",
       "          [[0.5372549 , 0.54117647, 0.58823529],\n",
       "           [0.53333333, 0.54117647, 0.59215686],\n",
       "           [0.5372549 , 0.54509804, 0.59215686],\n",
       "           ...,\n",
       "           [0.46666667, 0.45490196, 0.4627451 ],\n",
       "           [0.4627451 , 0.45490196, 0.45882353],\n",
       "           [0.46666667, 0.45490196, 0.46666667]],\n",
       " \n",
       "          [[0.52941176, 0.54509804, 0.58823529],\n",
       "           [0.52941176, 0.54509804, 0.59607843],\n",
       "           [0.53333333, 0.54901961, 0.59607843],\n",
       "           ...,\n",
       "           [0.4745098 , 0.46666667, 0.4745098 ],\n",
       "           [0.47843137, 0.46666667, 0.4745098 ],\n",
       "           [0.4745098 , 0.4627451 , 0.46666667]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.60392157, 0.5372549 , 0.45490196],\n",
       "           [0.60784314, 0.5372549 , 0.4627451 ],\n",
       "           [0.57254902, 0.50588235, 0.42745098],\n",
       "           ...,\n",
       "           [0.2745098 , 0.25490196, 0.2745098 ],\n",
       "           [0.28235294, 0.26666667, 0.27058824],\n",
       "           [0.29019608, 0.28235294, 0.27058824]],\n",
       " \n",
       "          [[0.61568627, 0.54117647, 0.47058824],\n",
       "           [0.61568627, 0.5372549 , 0.47058824],\n",
       "           [0.6       , 0.51764706, 0.45882353],\n",
       "           ...,\n",
       "           [0.30196078, 0.29019608, 0.30196078],\n",
       "           [0.29411765, 0.28235294, 0.28235294],\n",
       "           [0.28235294, 0.28235294, 0.26666667]],\n",
       " \n",
       "          [[0.60392157, 0.5254902 , 0.4745098 ],\n",
       "           [0.60784314, 0.5254902 , 0.4745098 ],\n",
       "           [0.59607843, 0.50588235, 0.4627451 ],\n",
       "           ...,\n",
       "           [0.27843137, 0.27058824, 0.27843137],\n",
       "           [0.2745098 , 0.27058824, 0.25882353],\n",
       "           [0.25882353, 0.25882353, 0.23921569]]]],\n",
       " \n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       " \n",
       "        [[[[0.65490196, 0.74901961, 0.66666667],\n",
       "           [0.65490196, 0.74901961, 0.6627451 ],\n",
       "           [0.65882353, 0.75294118, 0.6627451 ],\n",
       "           ...,\n",
       "           [0.54901961, 0.59607843, 0.51764706],\n",
       "           [0.54901961, 0.59215686, 0.51372549],\n",
       "           [0.54117647, 0.59215686, 0.50980392]],\n",
       " \n",
       "          [[0.65882353, 0.75294118, 0.67843137],\n",
       "           [0.6627451 , 0.75686275, 0.67058824],\n",
       "           [0.67058824, 0.76470588, 0.67058824],\n",
       "           ...,\n",
       "           [0.54117647, 0.58431373, 0.50588235],\n",
       "           [0.54509804, 0.58823529, 0.50980392],\n",
       "           [0.54117647, 0.58823529, 0.50980392]],\n",
       " \n",
       "          [[0.65882353, 0.75294118, 0.68627451],\n",
       "           [0.66666667, 0.76078431, 0.67843137],\n",
       "           [0.6745098 , 0.77254902, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.54901961, 0.6       , 0.52156863],\n",
       "           [0.54901961, 0.6       , 0.51764706],\n",
       "           [0.54509804, 0.59607843, 0.51372549]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.31372549, 0.32156863, 0.2627451 ],\n",
       "           [0.1372549 , 0.14901961, 0.09803922],\n",
       "           [0.04705882, 0.05490196, 0.01960784],\n",
       "           ...,\n",
       "           [0.65490196, 0.60784314, 0.5254902 ],\n",
       "           [0.65490196, 0.60784314, 0.52941176],\n",
       "           [0.65490196, 0.60784314, 0.5254902 ]],\n",
       " \n",
       "          [[0.25490196, 0.2627451 , 0.20392157],\n",
       "           [0.11372549, 0.1254902 , 0.0745098 ],\n",
       "           [0.05098039, 0.0627451 , 0.01568627],\n",
       "           ...,\n",
       "           [0.69411765, 0.6627451 , 0.56862745],\n",
       "           [0.68627451, 0.65490196, 0.56470588],\n",
       "           [0.68235294, 0.65098039, 0.56078431]],\n",
       " \n",
       "          [[0.14509804, 0.15686275, 0.09019608],\n",
       "           [0.07843137, 0.09019608, 0.02745098],\n",
       "           [0.04705882, 0.05882353, 0.00784314],\n",
       "           ...,\n",
       "           [0.72156863, 0.69803922, 0.59215686],\n",
       "           [0.70980392, 0.68627451, 0.58039216],\n",
       "           [0.70196078, 0.67843137, 0.57647059]]],\n",
       " \n",
       " \n",
       "         [[[0.63921569, 0.72156863, 0.64705882],\n",
       "           [0.64313725, 0.7254902 , 0.6627451 ],\n",
       "           [0.64313725, 0.7254902 , 0.65882353],\n",
       "           ...,\n",
       "           [0.54901961, 0.60392157, 0.51372549],\n",
       "           [0.54509804, 0.60392157, 0.51372549],\n",
       "           [0.54117647, 0.6       , 0.50980392]],\n",
       " \n",
       "          [[0.64705882, 0.72941176, 0.6627451 ],\n",
       "           [0.64705882, 0.72941176, 0.6627451 ],\n",
       "           [0.64705882, 0.72941176, 0.65882353],\n",
       "           ...,\n",
       "           [0.54509804, 0.6       , 0.50980392],\n",
       "           [0.54509804, 0.60392157, 0.51372549],\n",
       "           [0.54509804, 0.60392157, 0.51372549]],\n",
       " \n",
       "          [[0.6627451 , 0.74117647, 0.67843137],\n",
       "           [0.65882353, 0.74117647, 0.67058824],\n",
       "           [0.6627451 , 0.74509804, 0.67058824],\n",
       "           ...,\n",
       "           [0.54901961, 0.60392157, 0.51372549],\n",
       "           [0.54901961, 0.60392157, 0.51764706],\n",
       "           [0.54509804, 0.60392157, 0.51372549]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.34117647, 0.31764706, 0.29803922],\n",
       "           [0.17254902, 0.14117647, 0.1254902 ],\n",
       "           [0.08627451, 0.05882353, 0.04313725],\n",
       "           ...,\n",
       "           [0.65490196, 0.6       , 0.50980392],\n",
       "           [0.65490196, 0.6       , 0.51372549],\n",
       "           [0.65098039, 0.60392157, 0.52156863]],\n",
       " \n",
       "          [[0.27843137, 0.25490196, 0.22745098],\n",
       "           [0.14509804, 0.11372549, 0.09019608],\n",
       "           [0.0745098 , 0.04705882, 0.03529412],\n",
       "           ...,\n",
       "           [0.7372549 , 0.67843137, 0.59607843],\n",
       "           [0.7254902 , 0.67058824, 0.58431373],\n",
       "           [0.72156863, 0.6745098 , 0.58431373]],\n",
       " \n",
       "          [[0.15294118, 0.12941176, 0.10196078],\n",
       "           [0.13333333, 0.10588235, 0.07843137],\n",
       "           [0.10980392, 0.0745098 , 0.05098039],\n",
       "           ...,\n",
       "           [0.74117647, 0.67843137, 0.6       ],\n",
       "           [0.71764706, 0.6627451 , 0.58039216],\n",
       "           [0.70980392, 0.6627451 , 0.56862745]]],\n",
       " \n",
       " \n",
       "         [[[0.6627451 , 0.71764706, 0.65490196],\n",
       "           [0.6627451 , 0.71764706, 0.65882353],\n",
       "           [0.6627451 , 0.71764706, 0.65490196],\n",
       "           ...,\n",
       "           [0.54509804, 0.6       , 0.5372549 ],\n",
       "           [0.54117647, 0.60392157, 0.5372549 ],\n",
       "           [0.54117647, 0.59607843, 0.53333333]],\n",
       " \n",
       "          [[0.66666667, 0.72156863, 0.65882353],\n",
       "           [0.66666667, 0.72156863, 0.6627451 ],\n",
       "           [0.66666667, 0.72156863, 0.65882353],\n",
       "           ...,\n",
       "           [0.54509804, 0.6       , 0.53333333],\n",
       "           [0.54117647, 0.60392157, 0.53333333],\n",
       "           [0.54509804, 0.6       , 0.53333333]],\n",
       " \n",
       "          [[0.67843137, 0.73333333, 0.67058824],\n",
       "           [0.68235294, 0.7372549 , 0.6745098 ],\n",
       "           [0.67843137, 0.7372549 , 0.67058824],\n",
       "           ...,\n",
       "           [0.54117647, 0.6       , 0.52941176],\n",
       "           [0.54117647, 0.60392157, 0.52941176],\n",
       "           [0.54901961, 0.60392157, 0.53333333]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.30980392, 0.29019608, 0.24313725],\n",
       "           [0.16470588, 0.14509804, 0.09803922],\n",
       "           [0.0627451 , 0.03921569, 0.01568627],\n",
       "           ...,\n",
       "           [0.65098039, 0.6       , 0.51372549],\n",
       "           [0.65882353, 0.60392157, 0.51372549],\n",
       "           [0.65490196, 0.6       , 0.50980392]],\n",
       " \n",
       "          [[0.2627451 , 0.24313725, 0.19607843],\n",
       "           [0.14117647, 0.11764706, 0.07843137],\n",
       "           [0.05490196, 0.02745098, 0.00784314],\n",
       "           ...,\n",
       "           [0.73333333, 0.68235294, 0.59607843],\n",
       "           [0.7254902 , 0.6745098 , 0.58431373],\n",
       "           [0.72156863, 0.66666667, 0.58431373]],\n",
       " \n",
       "          [[0.19607843, 0.17647059, 0.12941176],\n",
       "           [0.13333333, 0.10588235, 0.0627451 ],\n",
       "           [0.07058824, 0.04313725, 0.01176471],\n",
       "           ...,\n",
       "           [0.7372549 , 0.68627451, 0.59607843],\n",
       "           [0.71764706, 0.66666667, 0.58039216],\n",
       "           [0.71372549, 0.65490196, 0.57647059]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.65490196, 0.74901961, 0.6627451 ],\n",
       "           [0.65490196, 0.74901961, 0.66666667],\n",
       "           [0.65882353, 0.75294118, 0.66666667],\n",
       "           ...,\n",
       "           [0.54901961, 0.59607843, 0.50980392],\n",
       "           [0.54509804, 0.59607843, 0.50588235],\n",
       "           [0.54509804, 0.59607843, 0.51372549]],\n",
       " \n",
       "          [[0.65882353, 0.75294118, 0.66666667],\n",
       "           [0.6627451 , 0.75686275, 0.67058824],\n",
       "           [0.67058824, 0.76470588, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.5372549 , 0.58431373, 0.49803922],\n",
       "           [0.5372549 , 0.58823529, 0.50588235],\n",
       "           [0.54117647, 0.58823529, 0.51372549]],\n",
       " \n",
       "          [[0.6627451 , 0.76078431, 0.66666667],\n",
       "           [0.67058824, 0.76470588, 0.67058824],\n",
       "           [0.67843137, 0.77254902, 0.68235294],\n",
       "           ...,\n",
       "           [0.54509804, 0.59607843, 0.51764706],\n",
       "           [0.54509804, 0.59215686, 0.51764706],\n",
       "           [0.54509804, 0.59215686, 0.52156863]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.28627451, 0.28627451, 0.24313725],\n",
       "           [0.1372549 , 0.14509804, 0.09019608],\n",
       "           [0.03921569, 0.05098039, 0.00784314],\n",
       "           ...,\n",
       "           [0.65098039, 0.59607843, 0.52156863],\n",
       "           [0.64705882, 0.59215686, 0.51764706],\n",
       "           [0.65098039, 0.6       , 0.52156863]],\n",
       " \n",
       "          [[0.23921569, 0.24313725, 0.19215686],\n",
       "           [0.1254902 , 0.13333333, 0.08627451],\n",
       "           [0.02352941, 0.03529412, 0.00392157],\n",
       "           ...,\n",
       "           [0.70588235, 0.67058824, 0.57647059],\n",
       "           [0.70196078, 0.6627451 , 0.56862745],\n",
       "           [0.69411765, 0.6627451 , 0.56862745]],\n",
       " \n",
       "          [[0.16078431, 0.16862745, 0.11372549],\n",
       "           [0.09803922, 0.10980392, 0.05882353],\n",
       "           [0.03529412, 0.04313725, 0.01176471],\n",
       "           ...,\n",
       "           [0.71764706, 0.69019608, 0.58431373],\n",
       "           [0.70588235, 0.67843137, 0.57254902],\n",
       "           [0.69411765, 0.6745098 , 0.56862745]]],\n",
       " \n",
       " \n",
       "         [[[0.6627451 , 0.71764706, 0.65490196],\n",
       "           [0.6627451 , 0.71764706, 0.65882353],\n",
       "           [0.6627451 , 0.71764706, 0.65882353],\n",
       "           ...,\n",
       "           [0.54509804, 0.6       , 0.51372549],\n",
       "           [0.54117647, 0.6       , 0.50980392],\n",
       "           [0.54117647, 0.6       , 0.50980392]],\n",
       " \n",
       "          [[0.6627451 , 0.71764706, 0.65490196],\n",
       "           [0.6627451 , 0.71764706, 0.65882353],\n",
       "           [0.66666667, 0.72156863, 0.65882353],\n",
       "           ...,\n",
       "           [0.54509804, 0.6       , 0.50980392],\n",
       "           [0.54117647, 0.6       , 0.50980392],\n",
       "           [0.54509804, 0.60392157, 0.51372549]],\n",
       " \n",
       "          [[0.67843137, 0.73333333, 0.67058824],\n",
       "           [0.6745098 , 0.73333333, 0.67058824],\n",
       "           [0.67843137, 0.72941176, 0.66666667],\n",
       "           ...,\n",
       "           [0.54509804, 0.60392157, 0.51372549],\n",
       "           [0.54509804, 0.60392157, 0.51372549],\n",
       "           [0.54509804, 0.60392157, 0.51372549]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.2745098 , 0.25098039, 0.22352941],\n",
       "           [0.16862745, 0.1372549 , 0.11764706],\n",
       "           [0.05882353, 0.02745098, 0.01176471],\n",
       "           ...,\n",
       "           [0.65098039, 0.6       , 0.51764706],\n",
       "           [0.65098039, 0.60392157, 0.52156863],\n",
       "           [0.65098039, 0.60392157, 0.52156863]],\n",
       " \n",
       "          [[0.23137255, 0.20784314, 0.18039216],\n",
       "           [0.10588235, 0.0745098 , 0.05490196],\n",
       "           [0.05490196, 0.02352941, 0.00392157],\n",
       "           ...,\n",
       "           [0.73333333, 0.68235294, 0.6       ],\n",
       "           [0.72156863, 0.6745098 , 0.59215686],\n",
       "           [0.71764706, 0.66666667, 0.58431373]],\n",
       " \n",
       "          [[0.16862745, 0.14901961, 0.11764706],\n",
       "           [0.0627451 , 0.03529412, 0.01960784],\n",
       "           [0.0745098 , 0.04313725, 0.01960784],\n",
       "           ...,\n",
       "           [0.73333333, 0.68627451, 0.6       ],\n",
       "           [0.71372549, 0.6627451 , 0.58431373],\n",
       "           [0.70980392, 0.65882353, 0.57647059]]],\n",
       " \n",
       " \n",
       "         [[[0.65490196, 0.75294118, 0.65490196],\n",
       "           [0.65490196, 0.75294118, 0.6627451 ],\n",
       "           [0.65882353, 0.75294118, 0.6627451 ],\n",
       "           ...,\n",
       "           [0.54509804, 0.59607843, 0.51372549],\n",
       "           [0.54117647, 0.59215686, 0.50588235],\n",
       "           [0.54117647, 0.59215686, 0.50980392]],\n",
       " \n",
       "          [[0.65882353, 0.75686275, 0.66666667],\n",
       "           [0.6627451 , 0.76078431, 0.67058824],\n",
       "           [0.67058824, 0.76470588, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.5372549 , 0.58431373, 0.49803922],\n",
       "           [0.54117647, 0.58823529, 0.50196078],\n",
       "           [0.54117647, 0.58823529, 0.50980392]],\n",
       " \n",
       "          [[0.66666667, 0.76078431, 0.6745098 ],\n",
       "           [0.67058824, 0.76470588, 0.67843137],\n",
       "           [0.67843137, 0.77254902, 0.68627451],\n",
       "           ...,\n",
       "           [0.54901961, 0.6       , 0.50980392],\n",
       "           [0.54901961, 0.6       , 0.51372549],\n",
       "           [0.54509804, 0.59607843, 0.51372549]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.31372549, 0.31764706, 0.27058824],\n",
       "           [0.12941176, 0.14509804, 0.09019608],\n",
       "           [0.03137255, 0.04705882, 0.00392157],\n",
       "           ...,\n",
       "           [0.64705882, 0.59607843, 0.52156863],\n",
       "           [0.64705882, 0.6       , 0.5254902 ],\n",
       "           [0.64313725, 0.6       , 0.52156863]],\n",
       " \n",
       "          [[0.28627451, 0.29411765, 0.24313725],\n",
       "           [0.10980392, 0.12156863, 0.07058824],\n",
       "           [0.01568627, 0.02745098, 0.        ],\n",
       "           ...,\n",
       "           [0.70588235, 0.67058824, 0.58431373],\n",
       "           [0.69803922, 0.66666667, 0.57647059],\n",
       "           [0.69019608, 0.65882353, 0.56470588]],\n",
       " \n",
       "          [[0.16862745, 0.18039216, 0.12941176],\n",
       "           [0.09803922, 0.10588235, 0.05882353],\n",
       "           [0.03137255, 0.03921569, 0.00784314],\n",
       "           ...,\n",
       "           [0.71764706, 0.69411765, 0.59215686],\n",
       "           [0.70196078, 0.67843137, 0.58039216],\n",
       "           [0.69019608, 0.67058824, 0.56470588]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[0.69411765, 0.77254902, 0.68235294],\n",
       "           [0.69411765, 0.77254902, 0.68235294],\n",
       "           [0.69411765, 0.77254902, 0.68235294],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.46666667],\n",
       "           [0.5254902 , 0.56078431, 0.4627451 ],\n",
       "           [0.5254902 , 0.56078431, 0.46666667]],\n",
       " \n",
       "          [[0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.68627451, 0.76470588, 0.6745098 ],\n",
       "           [0.69019608, 0.76862745, 0.67843137],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.47058824],\n",
       "           [0.5254902 , 0.56470588, 0.47058824],\n",
       "           [0.5254902 , 0.56078431, 0.46666667]],\n",
       " \n",
       "          [[0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.69019608, 0.76862745, 0.6745098 ],\n",
       "           [0.69019608, 0.76862745, 0.68235294],\n",
       "           ...,\n",
       "           [0.5254902 , 0.56470588, 0.4745098 ],\n",
       "           [0.52156863, 0.56470588, 0.4745098 ],\n",
       "           [0.51372549, 0.55686275, 0.4627451 ]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.25882353, 0.21176471, 0.12941176],\n",
       "           [0.35686275, 0.31764706, 0.24313725],\n",
       "           [0.6       , 0.54509804, 0.47843137],\n",
       "           ...,\n",
       "           [0.09411765, 0.10980392, 0.0745098 ],\n",
       "           [0.09803922, 0.10980392, 0.07843137],\n",
       "           [0.10588235, 0.11764706, 0.08627451]],\n",
       " \n",
       "          [[0.25882353, 0.21568627, 0.12941176],\n",
       "           [0.35294118, 0.31764706, 0.24313725],\n",
       "           [0.59607843, 0.54117647, 0.48235294],\n",
       "           ...,\n",
       "           [0.11372549, 0.12941176, 0.09411765],\n",
       "           [0.10980392, 0.1254902 , 0.09019608],\n",
       "           [0.10588235, 0.11764706, 0.08627451]],\n",
       " \n",
       "          [[0.25098039, 0.21960784, 0.12941176],\n",
       "           [0.34509804, 0.31764706, 0.23529412],\n",
       "           [0.59215686, 0.5372549 , 0.4745098 ],\n",
       "           ...,\n",
       "           [0.16470588, 0.18039216, 0.14117647],\n",
       "           [0.16862745, 0.18431373, 0.14901961],\n",
       "           [0.16862745, 0.18431373, 0.15294118]]],\n",
       " \n",
       " \n",
       "         [[[0.69411765, 0.77254902, 0.68627451],\n",
       "           [0.69019608, 0.76862745, 0.68627451],\n",
       "           [0.69019608, 0.77254902, 0.68627451],\n",
       "           ...,\n",
       "           [0.5254902 , 0.56078431, 0.48235294],\n",
       "           [0.5254902 , 0.56470588, 0.48235294],\n",
       "           [0.52941176, 0.56078431, 0.48627451]],\n",
       " \n",
       "          [[0.69411765, 0.76862745, 0.67843137],\n",
       "           [0.69019608, 0.76862745, 0.68235294],\n",
       "           [0.69019608, 0.76862745, 0.68235294],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.48235294],\n",
       "           [0.52941176, 0.56470588, 0.48627451],\n",
       "           [0.5254902 , 0.56078431, 0.48235294]],\n",
       " \n",
       "          [[0.69803922, 0.76470588, 0.67843137],\n",
       "           [0.69411765, 0.76862745, 0.68235294],\n",
       "           [0.69803922, 0.76862745, 0.68235294],\n",
       "           ...,\n",
       "           [0.53333333, 0.56078431, 0.48627451],\n",
       "           [0.5254902 , 0.56470588, 0.48627451],\n",
       "           [0.51764706, 0.55686275, 0.47843137]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.2627451 , 0.21176471, 0.12941176],\n",
       "           [0.35686275, 0.31764706, 0.24313725],\n",
       "           [0.60392157, 0.5372549 , 0.47843137],\n",
       "           ...,\n",
       "           [0.09411765, 0.10588235, 0.07058824],\n",
       "           [0.09411765, 0.10980392, 0.07058824],\n",
       "           [0.09803922, 0.11372549, 0.07058824]],\n",
       " \n",
       "          [[0.25882353, 0.21568627, 0.13333333],\n",
       "           [0.35294118, 0.31764706, 0.23529412],\n",
       "           [0.6       , 0.54117647, 0.4745098 ],\n",
       "           ...,\n",
       "           [0.10980392, 0.12941176, 0.09019608],\n",
       "           [0.10196078, 0.12156863, 0.08235294],\n",
       "           [0.10588235, 0.12156863, 0.08235294]],\n",
       " \n",
       "          [[0.25098039, 0.21176471, 0.12941176],\n",
       "           [0.35686275, 0.32156863, 0.23529412],\n",
       "           [0.59607843, 0.54509804, 0.4745098 ],\n",
       "           ...,\n",
       "           [0.16078431, 0.17647059, 0.14117647],\n",
       "           [0.16078431, 0.18431373, 0.14117647],\n",
       "           [0.16862745, 0.18431373, 0.14509804]]],\n",
       " \n",
       " \n",
       "         [[[0.69019608, 0.77647059, 0.68235294],\n",
       "           [0.69019608, 0.77254902, 0.6745098 ],\n",
       "           [0.69411765, 0.77647059, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.47843137],\n",
       "           [0.52941176, 0.56470588, 0.48627451],\n",
       "           [0.53333333, 0.56470588, 0.49019608]],\n",
       " \n",
       "          [[0.68627451, 0.77254902, 0.6745098 ],\n",
       "           [0.69019608, 0.77254902, 0.6745098 ],\n",
       "           [0.69019608, 0.77254902, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.5254902 , 0.56470588, 0.48627451],\n",
       "           [0.5254902 , 0.56470588, 0.48627451],\n",
       "           [0.52941176, 0.56470588, 0.49019608]],\n",
       " \n",
       "          [[0.68627451, 0.77254902, 0.67058824],\n",
       "           [0.69019608, 0.77647059, 0.6745098 ],\n",
       "           [0.69019608, 0.77647059, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.51764706, 0.55686275, 0.48235294],\n",
       "           [0.51372549, 0.55294118, 0.47843137],\n",
       "           [0.51372549, 0.55294118, 0.47843137]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.24705882, 0.20392157, 0.17254902],\n",
       "           [0.3372549 , 0.32156863, 0.27058824],\n",
       "           [0.60784314, 0.56862745, 0.50980392],\n",
       "           ...,\n",
       "           [0.08627451, 0.09803922, 0.05882353],\n",
       "           [0.09019608, 0.09411765, 0.05882353],\n",
       "           [0.09019608, 0.09411765, 0.05490196]],\n",
       " \n",
       "          [[0.24705882, 0.2       , 0.16470588],\n",
       "           [0.34509804, 0.3254902 , 0.27058824],\n",
       "           [0.6       , 0.54901961, 0.49019608],\n",
       "           ...,\n",
       "           [0.07843137, 0.11764706, 0.09411765],\n",
       "           [0.08627451, 0.12156863, 0.09803922],\n",
       "           [0.09019608, 0.1254902 , 0.10196078]],\n",
       " \n",
       "          [[0.24313725, 0.2       , 0.15294118],\n",
       "           [0.34901961, 0.32941176, 0.26666667],\n",
       "           [0.58823529, 0.53333333, 0.4745098 ],\n",
       "           ...,\n",
       "           [0.1372549 , 0.2       , 0.18431373],\n",
       "           [0.1254902 , 0.19215686, 0.17647059],\n",
       "           [0.12941176, 0.19215686, 0.17647059]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.69019608, 0.77647059, 0.68235294],\n",
       "           [0.69019608, 0.77254902, 0.6745098 ],\n",
       "           [0.69411765, 0.77647059, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.47843137],\n",
       "           [0.52941176, 0.56470588, 0.48627451],\n",
       "           [0.53333333, 0.56470588, 0.49019608]],\n",
       " \n",
       "          [[0.68627451, 0.77254902, 0.6745098 ],\n",
       "           [0.69019608, 0.76862745, 0.6745098 ],\n",
       "           [0.69019608, 0.77254902, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.5254902 , 0.56470588, 0.48627451],\n",
       "           [0.5254902 , 0.56470588, 0.48627451],\n",
       "           [0.52941176, 0.56470588, 0.49019608]],\n",
       " \n",
       "          [[0.68627451, 0.77254902, 0.67058824],\n",
       "           [0.69019608, 0.77647059, 0.6745098 ],\n",
       "           [0.69019608, 0.77647059, 0.6745098 ],\n",
       "           ...,\n",
       "           [0.51764706, 0.55686275, 0.48235294],\n",
       "           [0.51372549, 0.55294118, 0.47843137],\n",
       "           [0.51372549, 0.55294118, 0.47843137]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.24705882, 0.2       , 0.16862745],\n",
       "           [0.3372549 , 0.32156863, 0.26666667],\n",
       "           [0.61176471, 0.56862745, 0.50980392],\n",
       "           ...,\n",
       "           [0.08235294, 0.09803922, 0.06666667],\n",
       "           [0.08235294, 0.09411765, 0.05490196],\n",
       "           [0.09411765, 0.09411765, 0.05882353]],\n",
       " \n",
       "          [[0.24705882, 0.20392157, 0.16470588],\n",
       "           [0.34509804, 0.3254902 , 0.27058824],\n",
       "           [0.59607843, 0.54901961, 0.49019608],\n",
       "           ...,\n",
       "           [0.08627451, 0.12941176, 0.10588235],\n",
       "           [0.08627451, 0.11372549, 0.09803922],\n",
       "           [0.08627451, 0.12156863, 0.09803922]],\n",
       " \n",
       "          [[0.24313725, 0.2       , 0.16078431],\n",
       "           [0.34901961, 0.32941176, 0.2745098 ],\n",
       "           [0.58039216, 0.53333333, 0.47058824],\n",
       "           ...,\n",
       "           [0.16470588, 0.21960784, 0.20392157],\n",
       "           [0.14901961, 0.20392157, 0.19215686],\n",
       "           [0.1254902 , 0.18823529, 0.17254902]]],\n",
       " \n",
       " \n",
       "         [[[0.69411765, 0.77254902, 0.68235294],\n",
       "           [0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.69411765, 0.76862745, 0.68235294],\n",
       "           ...,\n",
       "           [0.52941176, 0.56862745, 0.47058824],\n",
       "           [0.52156863, 0.56470588, 0.46666667],\n",
       "           [0.5254902 , 0.56862745, 0.47058824]],\n",
       " \n",
       "          [[0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.68627451, 0.76862745, 0.6745098 ],\n",
       "           [0.69411765, 0.76862745, 0.67843137],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.4745098 ],\n",
       "           [0.5254902 , 0.56470588, 0.4745098 ],\n",
       "           [0.5254902 , 0.56470588, 0.4745098 ]],\n",
       " \n",
       "          [[0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.69019608, 0.77254902, 0.67843137],\n",
       "           [0.69019608, 0.77254902, 0.67843137],\n",
       "           ...,\n",
       "           [0.53333333, 0.56470588, 0.48627451],\n",
       "           [0.52941176, 0.56078431, 0.48235294],\n",
       "           [0.52156863, 0.55294118, 0.47058824]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.25882353, 0.20784314, 0.1372549 ],\n",
       "           [0.35686275, 0.32156863, 0.23921569],\n",
       "           [0.59607843, 0.54509804, 0.47058824],\n",
       "           ...,\n",
       "           [0.09411765, 0.10980392, 0.07843137],\n",
       "           [0.09803922, 0.11372549, 0.08235294],\n",
       "           [0.09019608, 0.11372549, 0.07843137]],\n",
       " \n",
       "          [[0.25882353, 0.21176471, 0.13333333],\n",
       "           [0.34901961, 0.32156863, 0.23529412],\n",
       "           [0.6       , 0.54117647, 0.4745098 ],\n",
       "           ...,\n",
       "           [0.11372549, 0.12156863, 0.09019608],\n",
       "           [0.10980392, 0.12156863, 0.09019608],\n",
       "           [0.09803922, 0.12156863, 0.08627451]],\n",
       " \n",
       "          [[0.25882353, 0.21568627, 0.12941176],\n",
       "           [0.34509804, 0.31372549, 0.22745098],\n",
       "           [0.6       , 0.5372549 , 0.47058824],\n",
       "           ...,\n",
       "           [0.16470588, 0.17647059, 0.14117647],\n",
       "           [0.16862745, 0.18431373, 0.15294118],\n",
       "           [0.16470588, 0.18823529, 0.15294118]]],\n",
       " \n",
       " \n",
       "         [[[0.69411765, 0.77254902, 0.68235294],\n",
       "           [0.69411765, 0.77254902, 0.68235294],\n",
       "           [0.69411765, 0.77254902, 0.68235294],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.46666667],\n",
       "           [0.5254902 , 0.56078431, 0.4627451 ],\n",
       "           [0.5254902 , 0.56078431, 0.46666667]],\n",
       " \n",
       "          [[0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.68627451, 0.76470588, 0.6745098 ],\n",
       "           [0.69019608, 0.76862745, 0.67843137],\n",
       "           ...,\n",
       "           [0.52941176, 0.56470588, 0.47058824],\n",
       "           [0.5254902 , 0.56470588, 0.47058824],\n",
       "           [0.5254902 , 0.56078431, 0.46666667]],\n",
       " \n",
       "          [[0.69019608, 0.76862745, 0.67843137],\n",
       "           [0.69019608, 0.76862745, 0.6745098 ],\n",
       "           [0.69019608, 0.76862745, 0.68235294],\n",
       "           ...,\n",
       "           [0.5254902 , 0.56470588, 0.4745098 ],\n",
       "           [0.52156863, 0.56470588, 0.4745098 ],\n",
       "           [0.51372549, 0.55686275, 0.4627451 ]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.25882353, 0.21176471, 0.12941176],\n",
       "           [0.35294118, 0.3254902 , 0.23921569],\n",
       "           [0.59607843, 0.54509804, 0.47058824],\n",
       "           ...,\n",
       "           [0.09019608, 0.10588235, 0.0745098 ],\n",
       "           [0.09803922, 0.10980392, 0.07843137],\n",
       "           [0.09803922, 0.11372549, 0.08235294]],\n",
       " \n",
       "          [[0.2627451 , 0.21176471, 0.13333333],\n",
       "           [0.35294118, 0.31764706, 0.23921569],\n",
       "           [0.59607843, 0.54509804, 0.4745098 ],\n",
       "           ...,\n",
       "           [0.11764706, 0.12941176, 0.09019608],\n",
       "           [0.10980392, 0.12156863, 0.08627451],\n",
       "           [0.09803922, 0.12156863, 0.08627451]],\n",
       " \n",
       "          [[0.2627451 , 0.21176471, 0.1372549 ],\n",
       "           [0.35686275, 0.31764706, 0.24313725],\n",
       "           [0.59607843, 0.54117647, 0.4745098 ],\n",
       "           ...,\n",
       "           [0.16470588, 0.17647059, 0.1372549 ],\n",
       "           [0.16862745, 0.18431373, 0.14509804],\n",
       "           [0.16470588, 0.18823529, 0.15294118]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[0.23529412, 0.18431373, 0.16862745],\n",
       "           [0.20784314, 0.16470588, 0.1372549 ],\n",
       "           [0.20784314, 0.17647059, 0.1372549 ],\n",
       "           ...,\n",
       "           [0.20392157, 0.20784314, 0.17647059],\n",
       "           [0.2       , 0.20784314, 0.17647059],\n",
       "           [0.20392157, 0.21176471, 0.18039216]],\n",
       " \n",
       "          [[0.21176471, 0.16862745, 0.14509804],\n",
       "           [0.21568627, 0.17647059, 0.14509804],\n",
       "           [0.22352941, 0.18823529, 0.15294118],\n",
       "           ...,\n",
       "           [0.20784314, 0.20392157, 0.18039216],\n",
       "           [0.20392157, 0.20392157, 0.18039216],\n",
       "           [0.20392157, 0.20784314, 0.18039216]],\n",
       " \n",
       "          [[0.21960784, 0.18039216, 0.14901961],\n",
       "           [0.21960784, 0.18823529, 0.16078431],\n",
       "           [0.23137255, 0.2       , 0.16862745],\n",
       "           ...,\n",
       "           [0.20784314, 0.2       , 0.17647059],\n",
       "           [0.2       , 0.19607843, 0.17647059],\n",
       "           [0.19607843, 0.2       , 0.18039216]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.16470588, 0.16470588, 0.09411765],\n",
       "           [0.30980392, 0.24313725, 0.16078431],\n",
       "           [0.42352941, 0.29411765, 0.18431373],\n",
       "           ...,\n",
       "           [0.09019608, 0.14117647, 0.18039216],\n",
       "           [0.09411765, 0.14509804, 0.17647059],\n",
       "           [0.08627451, 0.14117647, 0.16470588]],\n",
       " \n",
       "          [[0.17254902, 0.16862745, 0.10588235],\n",
       "           [0.31372549, 0.24705882, 0.17254902],\n",
       "           [0.42745098, 0.29803922, 0.18431373],\n",
       "           ...,\n",
       "           [0.08627451, 0.14117647, 0.18431373],\n",
       "           [0.08627451, 0.14509804, 0.18431373],\n",
       "           [0.07843137, 0.14117647, 0.16862745]],\n",
       " \n",
       "          [[0.16862745, 0.16470588, 0.10588235],\n",
       "           [0.29411765, 0.22352941, 0.14901961],\n",
       "           [0.42745098, 0.29803922, 0.18823529],\n",
       "           ...,\n",
       "           [0.09019608, 0.14117647, 0.18823529],\n",
       "           [0.08627451, 0.14509804, 0.18431373],\n",
       "           [0.07058824, 0.1372549 , 0.16862745]]],\n",
       " \n",
       " \n",
       "         [[[0.23921569, 0.19215686, 0.16470588],\n",
       "           [0.21960784, 0.17647059, 0.14509804],\n",
       "           [0.21176471, 0.18431373, 0.14901961],\n",
       "           ...,\n",
       "           [0.20784314, 0.20784314, 0.18039216],\n",
       "           [0.2       , 0.20784314, 0.18039216],\n",
       "           [0.20392157, 0.20784314, 0.18823529]],\n",
       " \n",
       "          [[0.21568627, 0.17254902, 0.14117647],\n",
       "           [0.21176471, 0.17254902, 0.14117647],\n",
       "           [0.21568627, 0.19215686, 0.15294118],\n",
       "           ...,\n",
       "           [0.20392157, 0.20784314, 0.18039216],\n",
       "           [0.2       , 0.20392157, 0.18039216],\n",
       "           [0.2       , 0.20392157, 0.18431373]],\n",
       " \n",
       "          [[0.21176471, 0.17647059, 0.14117647],\n",
       "           [0.21960784, 0.18823529, 0.15294118],\n",
       "           [0.22745098, 0.2       , 0.16470588],\n",
       "           ...,\n",
       "           [0.2       , 0.20392157, 0.18039216],\n",
       "           [0.2       , 0.20392157, 0.17647059],\n",
       "           [0.19607843, 0.2       , 0.18039216]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.16470588, 0.16470588, 0.09803922],\n",
       "           [0.29411765, 0.23921569, 0.16078431],\n",
       "           [0.41568627, 0.29411765, 0.18039216],\n",
       "           ...,\n",
       "           [0.08235294, 0.14901961, 0.18039216],\n",
       "           [0.08235294, 0.14901961, 0.18431373],\n",
       "           [0.0745098 , 0.14117647, 0.16862745]],\n",
       " \n",
       "          [[0.15686275, 0.15294118, 0.09411765],\n",
       "           [0.27843137, 0.21568627, 0.1372549 ],\n",
       "           [0.41176471, 0.29803922, 0.17647059],\n",
       "           ...,\n",
       "           [0.08627451, 0.14509804, 0.18823529],\n",
       "           [0.08627451, 0.14117647, 0.18823529],\n",
       "           [0.07843137, 0.1372549 , 0.17647059]],\n",
       " \n",
       "          [[0.12941176, 0.12941176, 0.06666667],\n",
       "           [0.23921569, 0.17647059, 0.09411765],\n",
       "           [0.40392157, 0.29019608, 0.16470588],\n",
       "           ...,\n",
       "           [0.09019608, 0.14117647, 0.2       ],\n",
       "           [0.09019608, 0.14117647, 0.2       ],\n",
       "           [0.07843137, 0.13333333, 0.18039216]]],\n",
       " \n",
       " \n",
       "         [[[0.23921569, 0.19215686, 0.17254902],\n",
       "           [0.20392157, 0.16470588, 0.1372549 ],\n",
       "           [0.20784314, 0.17647059, 0.1372549 ],\n",
       "           ...,\n",
       "           [0.20784314, 0.20784314, 0.18039216],\n",
       "           [0.20392157, 0.21176471, 0.18039216],\n",
       "           [0.2       , 0.20784314, 0.17647059]],\n",
       " \n",
       "          [[0.22352941, 0.18039216, 0.15686275],\n",
       "           [0.20784314, 0.16862745, 0.14117647],\n",
       "           [0.21176471, 0.18039216, 0.14509804],\n",
       "           ...,\n",
       "           [0.20392157, 0.20784314, 0.18039216],\n",
       "           [0.20392157, 0.21176471, 0.18039216],\n",
       "           [0.2       , 0.20784314, 0.18039216]],\n",
       " \n",
       "          [[0.20784314, 0.16862745, 0.1372549 ],\n",
       "           [0.21568627, 0.18039216, 0.14901961],\n",
       "           [0.22745098, 0.19607843, 0.16078431],\n",
       "           ...,\n",
       "           [0.19607843, 0.20784314, 0.17647059],\n",
       "           [0.2       , 0.20784314, 0.18039216],\n",
       "           [0.20392157, 0.20784314, 0.18823529]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.16078431, 0.16470588, 0.09411765],\n",
       "           [0.30980392, 0.24313725, 0.16470588],\n",
       "           [0.42352941, 0.29803922, 0.18039216],\n",
       "           ...,\n",
       "           [0.09411765, 0.14509804, 0.18039216],\n",
       "           [0.09019608, 0.14117647, 0.18039216],\n",
       "           [0.08627451, 0.1372549 , 0.17254902]],\n",
       " \n",
       "          [[0.16862745, 0.16862745, 0.10588235],\n",
       "           [0.31372549, 0.24705882, 0.16470588],\n",
       "           [0.42745098, 0.29803922, 0.18431373],\n",
       "           ...,\n",
       "           [0.08627451, 0.14117647, 0.18039216],\n",
       "           [0.08627451, 0.14509804, 0.18431373],\n",
       "           [0.07843137, 0.14117647, 0.17647059]],\n",
       " \n",
       "          [[0.16862745, 0.16470588, 0.10588235],\n",
       "           [0.29411765, 0.22745098, 0.14509804],\n",
       "           [0.43137255, 0.30196078, 0.19215686],\n",
       "           ...,\n",
       "           [0.09019608, 0.14117647, 0.18431373],\n",
       "           [0.09019608, 0.14901961, 0.18823529],\n",
       "           [0.07058824, 0.1372549 , 0.17647059]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.22745098, 0.21960784, 0.21176471],\n",
       "           [0.18431373, 0.16862745, 0.15686275],\n",
       "           [0.18823529, 0.16078431, 0.12941176],\n",
       "           ...,\n",
       "           [0.20392157, 0.2       , 0.17647059],\n",
       "           [0.19607843, 0.2       , 0.17254902],\n",
       "           [0.19607843, 0.2       , 0.17254902]],\n",
       " \n",
       "          [[0.23921569, 0.23137255, 0.22352941],\n",
       "           [0.2       , 0.18431373, 0.16862745],\n",
       "           [0.19607843, 0.17254902, 0.14117647],\n",
       "           ...,\n",
       "           [0.2       , 0.2       , 0.17254902],\n",
       "           [0.2       , 0.20392157, 0.17647059],\n",
       "           [0.2       , 0.2       , 0.17647059]],\n",
       " \n",
       "          [[0.21568627, 0.21176471, 0.19607843],\n",
       "           [0.19607843, 0.18431373, 0.16078431],\n",
       "           [0.19607843, 0.18039216, 0.14509804],\n",
       "           ...,\n",
       "           [0.19607843, 0.2       , 0.17647059],\n",
       "           [0.2       , 0.2       , 0.17647059],\n",
       "           [0.20392157, 0.19607843, 0.18039216]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.17254902, 0.15686275, 0.13333333],\n",
       "           [0.30588235, 0.23137255, 0.16078431],\n",
       "           [0.39215686, 0.28627451, 0.18039216],\n",
       "           ...,\n",
       "           [0.09019608, 0.14509804, 0.18823529],\n",
       "           [0.09019608, 0.14901961, 0.18823529],\n",
       "           [0.08235294, 0.1372549 , 0.18039216]],\n",
       " \n",
       "          [[0.18823529, 0.17254902, 0.14901961],\n",
       "           [0.31764706, 0.24313725, 0.17254902],\n",
       "           [0.39607843, 0.28627451, 0.18039216],\n",
       "           ...,\n",
       "           [0.09803922, 0.14901961, 0.2       ],\n",
       "           [0.09411765, 0.14901961, 0.2       ],\n",
       "           [0.08235294, 0.1372549 , 0.18823529]],\n",
       " \n",
       "          [[0.16862745, 0.14901961, 0.1254902 ],\n",
       "           [0.29019608, 0.21568627, 0.14117647],\n",
       "           [0.39607843, 0.28235294, 0.17647059],\n",
       "           ...,\n",
       "           [0.08627451, 0.14117647, 0.2       ],\n",
       "           [0.08627451, 0.14117647, 0.19607843],\n",
       "           [0.0745098 , 0.1254902 , 0.18039216]]],\n",
       " \n",
       " \n",
       "         [[[0.20392157, 0.2       , 0.18431373],\n",
       "           [0.18039216, 0.16470588, 0.14509804],\n",
       "           [0.18431373, 0.16078431, 0.12941176],\n",
       "           ...,\n",
       "           [0.2       , 0.2       , 0.17254902],\n",
       "           [0.19607843, 0.20392157, 0.17254902],\n",
       "           [0.18823529, 0.20392157, 0.17254902]],\n",
       " \n",
       "          [[0.22352941, 0.21960784, 0.20784314],\n",
       "           [0.20392157, 0.18823529, 0.16862745],\n",
       "           [0.2       , 0.17647059, 0.14509804],\n",
       "           ...,\n",
       "           [0.2       , 0.2       , 0.17254902],\n",
       "           [0.19607843, 0.20392157, 0.17254902],\n",
       "           [0.19215686, 0.20392157, 0.17647059]],\n",
       " \n",
       "          [[0.21176471, 0.20392157, 0.19607843],\n",
       "           [0.19607843, 0.18039216, 0.16862745],\n",
       "           [0.20392157, 0.18039216, 0.14901961],\n",
       "           ...,\n",
       "           [0.20392157, 0.20392157, 0.17254902],\n",
       "           [0.19607843, 0.2       , 0.17647059],\n",
       "           [0.2       , 0.20392157, 0.18431373]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.16078431, 0.14901961, 0.10980392],\n",
       "           [0.30980392, 0.23921569, 0.17647059],\n",
       "           [0.39607843, 0.28235294, 0.18431373],\n",
       "           ...,\n",
       "           [0.08235294, 0.14117647, 0.18823529],\n",
       "           [0.08627451, 0.14117647, 0.19215686],\n",
       "           [0.08235294, 0.1372549 , 0.18039216]],\n",
       " \n",
       "          [[0.18823529, 0.17647059, 0.12941176],\n",
       "           [0.3372549 , 0.25882353, 0.19607843],\n",
       "           [0.40392157, 0.28627451, 0.18431373],\n",
       "           ...,\n",
       "           [0.09411765, 0.14509804, 0.2       ],\n",
       "           [0.09019608, 0.14509804, 0.2       ],\n",
       "           [0.08627451, 0.13333333, 0.18431373]],\n",
       " \n",
       "          [[0.17254902, 0.16470588, 0.11764706],\n",
       "           [0.31372549, 0.23921569, 0.17647059],\n",
       "           [0.40784314, 0.28627451, 0.18039216],\n",
       "           ...,\n",
       "           [0.08235294, 0.1372549 , 0.19607843],\n",
       "           [0.08235294, 0.13333333, 0.19607843],\n",
       "           [0.0745098 , 0.11764706, 0.17647059]]],\n",
       " \n",
       " \n",
       "         [[[0.23921569, 0.18039216, 0.16470588],\n",
       "           [0.21176471, 0.16862745, 0.14117647],\n",
       "           [0.20784314, 0.18431373, 0.14117647],\n",
       "           ...,\n",
       "           [0.20784314, 0.20784314, 0.18039216],\n",
       "           [0.20392157, 0.20784314, 0.18039216],\n",
       "           [0.20392157, 0.20784314, 0.18039216]],\n",
       " \n",
       "          [[0.20784314, 0.16078431, 0.14117647],\n",
       "           [0.21960784, 0.18039216, 0.14901961],\n",
       "           [0.22352941, 0.2       , 0.15686275],\n",
       "           ...,\n",
       "           [0.20392157, 0.20392157, 0.17647059],\n",
       "           [0.19607843, 0.2       , 0.17254902],\n",
       "           [0.2       , 0.2       , 0.17647059]],\n",
       " \n",
       "          [[0.22352941, 0.19607843, 0.16862745],\n",
       "           [0.23137255, 0.2       , 0.17254902],\n",
       "           [0.23137255, 0.20392157, 0.17254902],\n",
       "           ...,\n",
       "           [0.2       , 0.20392157, 0.17254902],\n",
       "           [0.2       , 0.20392157, 0.17647059],\n",
       "           [0.20392157, 0.19607843, 0.18039216]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.16470588, 0.16862745, 0.10588235],\n",
       "           [0.31372549, 0.24705882, 0.17254902],\n",
       "           [0.42352941, 0.29803922, 0.19215686],\n",
       "           ...,\n",
       "           [0.07843137, 0.14901961, 0.18039216],\n",
       "           [0.07843137, 0.15294118, 0.17647059],\n",
       "           [0.0745098 , 0.14117647, 0.16862745]],\n",
       " \n",
       "          [[0.16078431, 0.15294118, 0.09803922],\n",
       "           [0.28627451, 0.20784314, 0.13333333],\n",
       "           [0.42745098, 0.29411765, 0.18039216],\n",
       "           ...,\n",
       "           [0.07843137, 0.14509804, 0.18823529],\n",
       "           [0.07843137, 0.14901961, 0.18431373],\n",
       "           [0.0745098 , 0.13333333, 0.17254902]],\n",
       " \n",
       "          [[0.12941176, 0.11764706, 0.05882353],\n",
       "           [0.23529412, 0.15686275, 0.08235294],\n",
       "           [0.41960784, 0.28235294, 0.16862745],\n",
       "           ...,\n",
       "           [0.08627451, 0.14117647, 0.19607843],\n",
       "           [0.08235294, 0.14509804, 0.19215686],\n",
       "           [0.0745098 , 0.13333333, 0.17647059]]]]]),\n",
       " array([[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = generator('./Project_data/val',val_doc,30)\n",
    "print(\"-------------Iteration-1---------------\")\n",
    "next(g)\n",
    "print(\"-------------Iteration-2---------------\")\n",
    "next(g)\n",
    "print(\"-------------Iteration-3---------------\")\n",
    "next(g)\n",
    "print(\"-------------Iteration-4---------------\")\n",
    "next(g)\n",
    "print(\"-------------Iteration-5---------------\")\n",
    "next(g)\n",
    "print(\"-------------Iteration-6---------------\")\n",
    "next(g)\n",
    "print(\"-------------Iteration-7---------------\")\n",
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your model here\n",
    "# create architecture\n",
    "\n",
    "# define parameters\n",
    "n_output = 5 # number of classes in case of classification, 1 in case of regression\n",
    "output_activation = 'softmax' # softmax or sigmoid in case of classification, linear in case of regression\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Conv3D(32,(2,2,2),activation='relu',padding='same',input_shape=(15,image_len,image_wid,3)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Conv3D(32,(2,2,2),activation='relu',padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Conv3D(64,(1,1,1),activation='relu',padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling3D(pool_size=(1,2,1)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Conv3D(128,(3,3,3),activation='relu',padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,1)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(GlobalAveragePooling3D())\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(512, activation = 'relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(256, activation = 'relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(5,activation=output_activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 15, 100, 100, 32)  800       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 100, 100, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 7, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 7, 50, 50, 32)     8224      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 50, 50, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 25, 25, 64)     2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 12, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 12, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 12, 25, 128)    221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 12, 25, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 6, 25, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 6, 25, 128)     0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 435,205\n",
      "Trainable params: 433,157\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.01)\n",
    "model1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = 'model_init_1' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "model_name2 = 'model_init_2' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/' \n",
    "model_name3 = 'model_init_3' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'    \n",
    "if not os.path.exists(model_name1):\n",
    "    os.mkdir(model_name1)\n",
    "if not os.path.exists(model_name2):\n",
    "    os.mkdir(model_name2)\n",
    "if not os.path.exists(model_name3):\n",
    "    os.mkdir(model_name3)\n",
    "        \n",
    "filepath1 = model_name1 + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "filepath2 = model_name2 + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "filepath3 = model_name3 + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(filepath1, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint3 = ModelCheckpoint(filepath3, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR =  ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "callbacks_list1 = [checkpoint1, LR]\n",
    "callbacks_list2 = [checkpoint2, LR]\n",
    "callbacks_list3 = [checkpoint3, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = int(num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size =Source path =  ./Project_data/train ; batch size = 30\n",
      "Epoch 1/30\n",
      " 30\n",
      "23/23 [==============================] - 44s 2s/step - loss: 1.9857 - categorical_accuracy: 0.3652 - val_loss: 6.1084 - val_categorical_accuracy: 0.1333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.10845, saving model to model_init_1_2020-09-1416_18_37.126384/model-00001-1.98573-0.36522-6.10845-0.13333.h5\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.3308 - categorical_accuracy: 0.3696 - val_loss: 3.1410 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.10845 to 3.14097, saving model to model_init_1_2020-09-1416_18_37.126384/model-00002-1.33081-0.36957-3.14097-0.23333.h5\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.1986 - categorical_accuracy: 0.4319 - val_loss: 2.6078 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.14097 to 2.60776, saving model to model_init_1_2020-09-1416_18_37.126384/model-00003-1.19856-0.43188-2.60776-0.25000.h5\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 1.1404 - categorical_accuracy: 0.4681 - val_loss: 1.3679 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.60776 to 1.36789, saving model to model_init_1_2020-09-1416_18_37.126384/model-00004-1.14037-0.46812-1.36789-0.38333.h5\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 1.1133 - categorical_accuracy: 0.4710 - val_loss: 1.6112 - val_categorical_accuracy: 0.3583\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.36789\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0529 - categorical_accuracy: 0.5029 - val_loss: 1.4319 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36789\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0776 - categorical_accuracy: 0.5087 - val_loss: 1.6147 - val_categorical_accuracy: 0.5083\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.36789\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9420 - categorical_accuracy: 0.5696 - val_loss: 1.7033 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.36789\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9489 - categorical_accuracy: 0.5754 - val_loss: 2.1182 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.36789\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9111 - categorical_accuracy: 0.5855 - val_loss: 1.6557 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.36789\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.7344 - categorical_accuracy: 0.6754 - val_loss: 1.5200 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.36789\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.7076 - categorical_accuracy: 0.6812 - val_loss: 1.6946 - val_categorical_accuracy: 0.2583\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.36789\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.6393 - categorical_accuracy: 0.6957 - val_loss: 1.6214 - val_categorical_accuracy: 0.4917\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.36789\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.6804 - categorical_accuracy: 0.6899 - val_loss: 1.6150 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.36789\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.6628 - categorical_accuracy: 0.7000 - val_loss: 1.7078 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.36789\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - 35s 2s/step - loss: 0.5967 - categorical_accuracy: 0.7203 - val_loss: 1.4413 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.36789\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.5827 - categorical_accuracy: 0.7275 - val_loss: 1.7577 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.36789\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5992 - categorical_accuracy: 0.7261 - val_loss: 1.4789 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.36789\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5836 - categorical_accuracy: 0.7203 - val_loss: 1.2805 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.36789 to 1.28046, saving model to model_init_1_2020-09-1416_18_37.126384/model-00019-0.58356-0.72029-1.28046-0.43333.h5\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5611 - categorical_accuracy: 0.7377 - val_loss: 1.5108 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.28046\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.5204 - categorical_accuracy: 0.7725 - val_loss: 1.7546 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.28046\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.6217 - categorical_accuracy: 0.7159 - val_loss: 1.7352 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.28046\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5386 - categorical_accuracy: 0.7464 - val_loss: 1.6205 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.28046\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5497 - categorical_accuracy: 0.7420 - val_loss: 1.5349 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.28046\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.4942 - categorical_accuracy: 0.7812 - val_loss: 1.5809 - val_categorical_accuracy: 0.5333\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.28046\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5547 - categorical_accuracy: 0.7348 - val_loss: 1.4796 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.28046\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5000 - categorical_accuracy: 0.7870 - val_loss: 1.5722 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.28046\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.4847 - categorical_accuracy: 0.7812 - val_loss: 1.3721 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.28046\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5151 - categorical_accuracy: 0.7754 - val_loss: 1.4196 - val_categorical_accuracy: 0.4417\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.28046\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.5025 - categorical_accuracy: 0.7406 - val_loss: 1.4734 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.28046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faffafada58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list1, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL-2 (CNN+RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 15, 4608)          14714688  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 15, 128)           2425344   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 17,280,453\n",
      "Trainable params: 2,565,637\n",
      "Non-trainable params: 14,714,816\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "n_input=  (image_len,image_wid,3)\n",
    "n_output = 5 # number of classes in case of classification, 1 in case of regression\n",
    "output_activation = 'softmax'\n",
    "\n",
    "\n",
    "cnn = VGG16(include_top=False,input_shape=n_input)\n",
    "\n",
    "for layer in cnn.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "flat1= Flatten()(cnn.layers[-1].output)\n",
    "cnn = Model(inputs=cnn.inputs, outputs=flat1)\n",
    "model2= Sequential()\n",
    "model2.add(TimeDistributed(cnn,input_shape=(15,image_len,image_wid,3)))\n",
    "model2.add(LSTM(128, dropout=0.25,return_sequences=True))\n",
    "model2.add(LSTM(128, dropout=0.25))\n",
    "model2.add(Dense(64, activation = \"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(5, activation = \"softmax\"))\n",
    "optimiser = optimizers.Adam(lr=0.01)\n",
    "model2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23/23 [==============================] - 49s 2s/step - loss: 1.7462 - categorical_accuracy: 0.2101 - val_loss: 2.4471 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.44708, saving model to model_init_2_2020-09-1416_18_37.126384/model-00001-1.74616-0.21014-2.44708-0.20000.h5\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.4591 - categorical_accuracy: 0.2957 - val_loss: 2.0703 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.44708 to 2.07034, saving model to model_init_2_2020-09-1416_18_37.126384/model-00002-1.45915-0.29565-2.07034-0.23333.h5\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - 39s 2s/step - loss: 1.3570 - categorical_accuracy: 0.3609 - val_loss: 2.2794 - val_categorical_accuracy: 0.3083\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.07034\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - 39s 2s/step - loss: 1.3060 - categorical_accuracy: 0.3594 - val_loss: 2.6821 - val_categorical_accuracy: 0.1750\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.07034\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 1.1920 - categorical_accuracy: 0.4449 - val_loss: 2.4852 - val_categorical_accuracy: 0.2417\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.07034\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 1.2790 - categorical_accuracy: 0.3942 - val_loss: 2.1241 - val_categorical_accuracy: 0.1417\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.07034\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.2711 - categorical_accuracy: 0.4000 - val_loss: 2.8091 - val_categorical_accuracy: 0.1750\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.07034\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 1.1312 - categorical_accuracy: 0.4623 - val_loss: 1.5215 - val_categorical_accuracy: 0.2833\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.07034 to 1.52149, saving model to model_init_2_2020-09-1416_18_37.126384/model-00008-1.13117-0.46232-1.52149-0.28333.h5\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.1168 - categorical_accuracy: 0.4884 - val_loss: 1.3327 - val_categorical_accuracy: 0.2583\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.52149 to 1.33270, saving model to model_init_2_2020-09-1416_18_37.126384/model-00009-1.11675-0.48841-1.33270-0.25833.h5\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.0550 - categorical_accuracy: 0.5275 - val_loss: 1.2355 - val_categorical_accuracy: 0.3167\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.33270 to 1.23552, saving model to model_init_2_2020-09-1416_18_37.126384/model-00010-1.05500-0.52754-1.23552-0.31667.h5\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.0369 - categorical_accuracy: 0.5435 - val_loss: 1.1933 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.23552 to 1.19327, saving model to model_init_2_2020-09-1416_18_37.126384/model-00011-1.03687-0.54348-1.19327-0.35000.h5\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.9665 - categorical_accuracy: 0.5609 - val_loss: 1.2183 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.19327\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9668 - categorical_accuracy: 0.5638 - val_loss: 1.3117 - val_categorical_accuracy: 0.3917\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.19327\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9331 - categorical_accuracy: 0.5725 - val_loss: 1.3598 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.19327\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9088 - categorical_accuracy: 0.6087 - val_loss: 1.7457 - val_categorical_accuracy: 0.2417\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.19327\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.8518 - categorical_accuracy: 0.6275 - val_loss: 1.0910 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.19327 to 1.09101, saving model to model_init_2_2020-09-1416_18_37.126384/model-00016-0.85178-0.62754-1.09101-0.38333.h5\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.8351 - categorical_accuracy: 0.6203 - val_loss: 1.0571 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.09101 to 1.05705, saving model to model_init_2_2020-09-1416_18_37.126384/model-00017-0.83510-0.62029-1.05705-0.46667.h5\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.8000 - categorical_accuracy: 0.6522 - val_loss: 1.5149 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05705\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.8449 - categorical_accuracy: 0.6072 - val_loss: 1.3622 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05705\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.7705 - categorical_accuracy: 0.6391 - val_loss: 1.0041 - val_categorical_accuracy: 0.4583\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.05705 to 1.00412, saving model to model_init_2_2020-09-1416_18_37.126384/model-00020-0.77049-0.63913-1.00412-0.45833.h5\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.7463 - categorical_accuracy: 0.6449 - val_loss: 1.1231 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.00412\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.6915 - categorical_accuracy: 0.6841 - val_loss: 1.2077 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.00412\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.7141 - categorical_accuracy: 0.6971 - val_loss: 1.1540 - val_categorical_accuracy: 0.4417\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.00412\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.7112 - categorical_accuracy: 0.6870 - val_loss: 1.1365 - val_categorical_accuracy: 0.4417\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.00412\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.7201 - categorical_accuracy: 0.6507 - val_loss: 1.3152 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.00412\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.6334 - categorical_accuracy: 0.7087 - val_loss: 1.3364 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.00412\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.6485 - categorical_accuracy: 0.7217 - val_loss: 1.0755 - val_categorical_accuracy: 0.4417\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.00412\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.6190 - categorical_accuracy: 0.7304 - val_loss: 1.1420 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.00412\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.6560 - categorical_accuracy: 0.7116 - val_loss: 1.1019 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.00412\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.6106 - categorical_accuracy: 0.6942 - val_loss: 1.0060 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.00412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafc26ab978>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list2, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model -3 (Modified Conv 3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_5 (Conv3D)            (None, 15, 100, 100, 32)  12032     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 15, 100, 100, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 7, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 7, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 7, 50, 50, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 50, 50, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 3, 25, 25, 64)     16448     \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 3, 25, 25, 128)    8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 25, 25, 128)    512       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3, 25, 25, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 3, 25, 25, 128)    442496    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 3, 25, 25, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 1, 12, 25, 128)    0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 12, 25, 128)    0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 709,989\n",
      "Trainable params: 707,813\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "# create architecture\n",
    "\n",
    "# define parameters\n",
    "n_output = 5 # number of classes in case of classification, 1 in case of regression\n",
    "output_activation = 'softmax' # softmax or sigmoid in case of classification, linear in case of regression\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Conv3D(32,(5,5,5),activation='relu',padding='same',input_shape=(15,image_len,image_wid,3)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(32,(3,3,3),activation='relu',padding='same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(64,(2,2,2),activation='relu',padding='same'))\n",
    "\n",
    "model3.add(Conv3D(128,(1,1,1),activation='relu',padding='same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(128,(3,3,3),activation='relu',padding='same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,1)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(GlobalAveragePooling3D())\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(512, activation = 'relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(256, activation = 'relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(5,activation=output_activation))\n",
    "optimiser = optimizers.SGD(lr=0.01)\n",
    "model3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.7154 - categorical_accuracy: 0.3130 - val_loss: 2.1756 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17556, saving model to model_init_3_2020-09-1416_18_37.126384/model-00001-1.71539-0.31304-2.17556-0.15000.h5\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.4685 - categorical_accuracy: 0.3725 - val_loss: 4.5877 - val_categorical_accuracy: 0.1167\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.17556\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - 34s 1s/step - loss: 1.3881 - categorical_accuracy: 0.4029 - val_loss: 3.8657 - val_categorical_accuracy: 0.1167\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.17556\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.2787 - categorical_accuracy: 0.4217 - val_loss: 3.7790 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.17556\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.2532 - categorical_accuracy: 0.4536 - val_loss: 4.2468 - val_categorical_accuracy: 0.2083\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.17556\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.1276 - categorical_accuracy: 0.4652 - val_loss: 2.8629 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.17556\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.1480 - categorical_accuracy: 0.4870 - val_loss: 3.2517 - val_categorical_accuracy: 0.3083\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.17556\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.1289 - categorical_accuracy: 0.4783 - val_loss: 2.1866 - val_categorical_accuracy: 0.3417\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.17556\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.0690 - categorical_accuracy: 0.4899 - val_loss: 2.7272 - val_categorical_accuracy: 0.3083\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.17556\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 1.0169 - categorical_accuracy: 0.5043 - val_loss: 2.5775 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.17556\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - 35s 2s/step - loss: 1.0704 - categorical_accuracy: 0.4986 - val_loss: 2.5285 - val_categorical_accuracy: 0.3583\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.17556\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0669 - categorical_accuracy: 0.4913 - val_loss: 2.1204 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.17556 to 2.12040, saving model to model_init_3_2020-09-1416_18_37.126384/model-00012-1.06687-0.49130-2.12040-0.38333.h5\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0628 - categorical_accuracy: 0.4652 - val_loss: 2.1962 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.12040\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0217 - categorical_accuracy: 0.5188 - val_loss: 2.2400 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.12040\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9631 - categorical_accuracy: 0.5478 - val_loss: 2.1764 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.12040\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0744 - categorical_accuracy: 0.4826 - val_loss: 2.3363 - val_categorical_accuracy: 0.3583\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.12040\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9948 - categorical_accuracy: 0.5449 - val_loss: 2.3657 - val_categorical_accuracy: 0.3167\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.12040\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9137 - categorical_accuracy: 0.5826 - val_loss: 2.5272 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.12040\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9743 - categorical_accuracy: 0.5464 - val_loss: 2.1113 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.12040 to 2.11134, saving model to model_init_3_2020-09-1416_18_37.126384/model-00019-0.97432-0.54638-2.11134-0.41667.h5\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 1.0188 - categorical_accuracy: 0.5275 - val_loss: 2.5592 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.11134\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9987 - categorical_accuracy: 0.5261 - val_loss: 2.9147 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.11134\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9587 - categorical_accuracy: 0.5348 - val_loss: 2.0621 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.11134 to 2.06210, saving model to model_init_3_2020-09-1416_18_37.126384/model-00022-0.95875-0.53478-2.06210-0.43333.h5\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9502 - categorical_accuracy: 0.5609 - val_loss: 2.1087 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.06210\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.8989 - categorical_accuracy: 0.5754 - val_loss: 2.0614 - val_categorical_accuracy: 0.3917\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.06210 to 2.06142, saving model to model_init_3_2020-09-1416_18_37.126384/model-00024-0.89895-0.57536-2.06142-0.39167.h5\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9701 - categorical_accuracy: 0.5406 - val_loss: 2.0462 - val_categorical_accuracy: 0.3917\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.06142 to 2.04616, saving model to model_init_3_2020-09-1416_18_37.126384/model-00025-0.97012-0.54058-2.04616-0.39167.h5\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9034 - categorical_accuracy: 0.5797 - val_loss: 1.9752 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.04616 to 1.97516, saving model to model_init_3_2020-09-1416_18_37.126384/model-00026-0.90338-0.57971-1.97516-0.40000.h5\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9515 - categorical_accuracy: 0.5464 - val_loss: 2.4653 - val_categorical_accuracy: 0.3417\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.97516\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9540 - categorical_accuracy: 0.5377 - val_loss: 2.2824 - val_categorical_accuracy: 0.3917\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.97516\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.9632 - categorical_accuracy: 0.5449 - val_loss: 2.3934 - val_categorical_accuracy: 0.4083\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.97516\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.9059 - categorical_accuracy: 0.5913 - val_loss: 2.0272 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.97516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafdd4b3e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list3, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
